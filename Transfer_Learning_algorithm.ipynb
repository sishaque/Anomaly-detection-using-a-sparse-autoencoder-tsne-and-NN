{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer_Learning_algorithm",
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1GPssq6cg1LfWki3klP-ctjSJGI-ldsNC",
      "authorship_tag": "ABX9TyPq9R2jiv1qVSHgXK7byvTL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sishaque/Anomaly-detection-using-a-sparse-autoencoder-tsne-and-NN/blob/main/Transfer_Learning_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj7eFxO0WitB"
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import biosppy\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import sys\n",
        "import os\n",
        "from keras.preprocessing import image\n",
        "import wfdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGKJ9Wf5KGqZ"
      },
      "source": [
        "pip install wfdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zTYtTWZw7Kv"
      },
      "source": [
        "pip install biosppy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MWJtKeZIdco"
      },
      "source": [
        "df = pd.read_csv(\"/content/U12T3_dataECG.csv\", sep=',', usecols= [2])\n",
        "df = df*-1\n",
        "print(df)\n",
        "df = np.transpose(df)\n",
        "ECG_data = np.array(df)\n",
        "ECG_data = ECG_data.flatten()\n",
        "print(ECG_data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9ldUkC0Hr6G"
      },
      "source": [
        "def segmentation(records):\n",
        "    Normal = []\n",
        "    for e in records:\n",
        "        signals, fields = wfdb.rdsamp(e, channels = [0]) \n",
        "\n",
        "        ann = wfdb.rdann(e, '/content/Saved_dataECG (4).csv')\n",
        "        good = ['N']\n",
        "        ids = np.in1d(ann.symbol, good)\n",
        "        imp_beats = ann.sample[ids]\n",
        "        beats = (ann.sample)\n",
        "        for i in imp_beats:\n",
        "            beats = list(beats)\n",
        "            j = beats.index(i)\n",
        "            if(j!=0 and j!=(len(beats)-1)):\n",
        "                x = beats[j-1]\n",
        "                y = beats[j+1]\n",
        "                diff1 = abs(x - beats[j])//2\n",
        "                diff2 = abs(y - beats[j])//2\n",
        "                Normal.append(signals[beats[j] - diff1: beats[j] + diff2, 0])\n",
        "    return Normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VveUiNshTbAN"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNZ47I640F-S"
      },
      "source": [
        "def ECG_signal(csv_data):\n",
        "  data = np.array(csv_data)\n",
        "  signals = []\n",
        "  count = 1\n",
        "  peaks =  biosppy.signals.ecg.christov_segmenter(signal=data, sampling_rate = 200)[0]\n",
        "  for i in (peaks[1:-1]):\n",
        "    diff1 = abs(peaks[count - 1] - i)\n",
        "    diff2 = abs(peaks[count + 1]- i)\n",
        "    x = peaks[count - 1] + diff1//2\n",
        "    y = peaks[count + 1] - diff2//2\n",
        "    signal = data[x:y]\n",
        "    signals.append(signal)\n",
        "    count += 1\n",
        "  return signals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU2sdt6rfCqj"
      },
      "source": [
        "# data = segmentation(df)\n",
        "Signal = ECG_signal(ECG_data)\n",
        "\n",
        "for count, i in enumerate(Signal):\n",
        "  fig = plt.figure(frameon=False)\n",
        "  plt.plot(i) \n",
        "  plt.xticks([]), plt.yticks([])\n",
        "  for spine in plt.gca().spines.values():\n",
        "     spine.set_visible(False)\n",
        "\n",
        "  filename = '/content/drive/MyDrive/ECG_stress_image_dataset/stressed/' + str(2939+count)+'.png'\n",
        "  plt.savefig(filename)\n",
        "  im_gray = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
        "  im_gray = cv2.resize(im_gray, (128, 128), interpolation = cv2.INTER_LANCZOS4)\n",
        "  cv2.imwrite(filename, im_gray)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk_Gru_OqX6G"
      },
      "source": [
        "pip install pigeon-jupyter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt8e4mxkdzC2"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "not_stressed_dataset_dir = '/content/drive/MyDrive/ECG_stress_image_dataset/not_stressed'\n",
        "not_stressed_train = os.path.join(not_stressed_dataset_dir, 'not_stressed_train')\n",
        "# os.mkdir(not_stressed_train)\n",
        "\n",
        "fnames = ['{}.png'.format(i) for i in range(1800)]\n",
        "for fname in fnames:\n",
        "  src = os.path.join(not_stressed_dataset_dir, fname)\n",
        "  dst = os.path.join(not_stressed_train, fname)\n",
        "  shutil.copyfile(src,dst)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPUBL6R0zB7L"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "stressed_dataset_dir = '/content/drive/MyDrive/ECG_stress_image_dataset/stressed'\n",
        "stress_train_dir = os.path.join('/content/drive/MyDrive/ECG_stress_image_dataset/', 'stress_train_dir')\n",
        "# os.mkdir(stress_train_dir)\n",
        "\n",
        "fnames = ['{}.png'.format(i+2001) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "  src = os.path.join(stressed_dataset_dir, fname)\n",
        "  dst = os.path.join(stress_train_dir, fname)\n",
        "  shutil.copyfile(src,dst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A1bXEuhmVKV"
      },
      "source": [
        "stressed_dataset_dir = '/content/drive/MyDrive/ECG_stress_image_dataset/stressed'\n",
        "stress_test_dir = os.path.join('/content/drive/MyDrive/ECG_stress_image_dataset/', 'stress_test_dir')\n",
        "# os.mkdir(stress_test_dir)\n",
        "\n",
        "fnames = ['{}.png'.format(i+501) for i in range(500)]\n",
        "for fname in fnames:\n",
        "  src = os.path.join(stressed_dataset_dir, fname)\n",
        "  dst = os.path.join(stress_test_dir, fname)\n",
        "  shutil.copyfile(src,dst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfTlnx0_1cty"
      },
      "source": [
        "stress_train_dir = '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir'\n",
        "stress_val_dir = '/content/drive/MyDrive/ECG_stress_image_dataset/stress_val_dir'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zva7ER58reA4"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (128, 128, 3)))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Conv2D(128, (3,3), activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation = 'relu'))\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr = 1e-4), metrics = ['acc'])\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    stress_train_dir,\n",
        "    target_size = (128, 128),\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    stress_val_dir,\n",
        "    target_size = (128, 128),\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary')\n",
        "\n",
        "\n",
        "\n",
        "for data_batch, labels_batch in train_generator:\n",
        "  print('data batch shape:', data_batch.shape)\n",
        "  print('labels batch shape', labels_batch.shape)\n",
        "  break\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch = 100,\n",
        "                              epochs = 30,\n",
        "                              validation_data = validation_generator,\n",
        "                              validation_steps = 50)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label = 'Trainning acc')\n",
        "plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
        "plt.title('Trainning and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label = 'Trainning loss')\n",
        "plt.plot(epochs, val_acc, 'b', label = 'Validation loss')\n",
        "plt.title('Trainning and validation loss')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQEWZ7gONJex"
      },
      "source": [
        "pip install pigeon-jupyter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouD9z3_5qc_M"
      },
      "source": [
        "from pigeon import annotate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmGp5ctPxLve"
      },
      "source": [
        "from IPython.display import display, Image\n",
        "\n",
        "annotations = annotate(\n",
        "    ['/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/0.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/1.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/2.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/3.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/4.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/5.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/6.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/7.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/8.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/9.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/10.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/11.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/12.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/13.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/14.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/15.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/16.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/17.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/18.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/19.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/20.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/21.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/22.png',\n",
        "     '/content/drive/MyDrive/ECG_stress_image_dataset/stress_train_dir/23.png'],\n",
        "    options = ['not stressed', 'stressed' ],\n",
        "    display_fn = lambda filename: display(Image(filename))\n",
        "    \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iLim3N0R3eT"
      },
      "source": [
        "for data, label in annotations:\n",
        "  print(\"data\", data)\n",
        "  print(\"label\", label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kOgo9ahqoY5"
      },
      "source": [
        "annotations\n",
        "\n",
        "x, y = image.img_to_array(annotations)\n",
        "\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFdUHq9zxTKT"
      },
      "source": [
        "import os\n",
        "import h5py\n",
        "import wfdb as wf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from scipy import signal as ss\n",
        "from utils import download as ud\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def get_records():\n",
        "    \"\"\" Get paths for data in data/mit/ directory \"\"\"\n",
        "    # Download if doesn't exist\n",
        "    if not os.path.isdir('data/mitdb'):\n",
        "        print 'Downloading the mitdb ecg database, please wait'\n",
        "        ud.download_mitdb()\n",
        "        print 'Download finished'\n",
        "\n",
        "    # There are 3 files for each record\n",
        "    # *.atr is one of them\n",
        "    paths = glob('data/mitdb/*.atr')\n",
        "\n",
        "    # Get rid of the extension\n",
        "    paths = [path[:-4] for path in paths]\n",
        "    paths.sort()\n",
        "\n",
        "    return paths\n",
        "\n",
        "def good_types():\n",
        "    \"\"\" Of annotations \"\"\"\n",
        "    # www.physionet.org/physiobank/annotations.shtml\n",
        "    good = ['N', 'L', 'R', 'B', 'A',\n",
        "            'a', 'J', 'S', 'V', 'r',\n",
        "            'F', 'e', 'j', 'n', 'E',\n",
        "            '/', 'f', 'Q', '?']\n",
        "\n",
        "    return good\n",
        "\n",
        "def beat_annotations(annotation):\n",
        "    \"\"\" Get rid of non-beat markers \"\"\"\n",
        "    # Declare beat types\n",
        "    good = good_types()\n",
        "    ids = np.in1d(annotation.anntype, good)\n",
        "\n",
        "    # We want to know only the positions\n",
        "    beats = annotation.annsamp[ids]\n",
        "\n",
        "    return beats\n",
        "\n",
        "def convert_input(channel, annotation):\n",
        "    \"\"\" Into output \"\"\"\n",
        "    # Remove non-beat annotations\n",
        "    beats = beat_annotations(annotation)\n",
        "\n",
        "    # Create dirac-comb signal\n",
        "    dirac = np.zeros_like(channel)\n",
        "    dirac[beats] = 1.0\n",
        "\n",
        "    # Use hamming window as a bell-curve filter\n",
        "    width = 36\n",
        "    filter = ss.hamming(width)\n",
        "    gauss = np.convolve(filter, dirac, mode = 'same')\n",
        "\n",
        "    return dirac, gauss\n",
        "\n",
        "def good_annotations():\n",
        "    \"\"\" Const function with good annotations \"\"\"\n",
        "    # For now it seems those are most popular\n",
        "    good_annotations = [1, 2, 3, 4,\n",
        "                        5, 6, 7, 8,\n",
        "                        9, 10, 11, 12,\n",
        "                        13, 16, 31, 38]\n",
        "\n",
        "    return good_annotations\n",
        "\n",
        "def make_dataset(records, width, savepath):\n",
        "    \"\"\" Inside an array \"\"\"\n",
        "    # Prepare containers\n",
        "    signals, labels = [], []\n",
        "\n",
        "    # Iterate files\n",
        "    for path in records:\n",
        "        print 'Processing file:', path\n",
        "        record = wf.rdsamp(path)\n",
        "        annotations = wf.rdann(path, 'atr')\n",
        "\n",
        "        # Extract pure signals\n",
        "        data = record.p_signals\n",
        "\n",
        "        # Convert each channel into labeled fragments\n",
        "        signal, label = convert_data(data, annotations, width)\n",
        "\n",
        "        # Cumulate\n",
        "        signals.append(signal)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert to one huge numpy.array\n",
        "    signals = np.vstack(signals)\n",
        "    labels = np.vstack(labels)\n",
        "\n",
        "    # Write to disk\n",
        "    np.save(savepath, {'signals' : signals,\n",
        "                       'labels'  : labels })\n",
        "\n",
        "def convert_data(data, annotations, width):\n",
        "    \"\"\" Into a batch \"\"\"\n",
        "    # Prepare containers\n",
        "    signals, labels = [], []\n",
        "\n",
        "    # Convert both channels\n",
        "    for it in range(2):\n",
        "        channel = data[:, it]\n",
        "        dirac, gauss = convert_input(channel,\n",
        "                                     annotations)\n",
        "        # Merge labels\n",
        "        label = np.vstack([dirac, gauss])\n",
        "\n",
        "        # Prepare the moving window\n",
        "        sta = 0\n",
        "        end = width\n",
        "        stride = width\n",
        "        while end <= len(channel):\n",
        "            # Chop out the fragments\n",
        "            s_frag = channel[sta : end]\n",
        "            l_frag = label[:, sta : end]\n",
        "\n",
        "            # Cumulate\n",
        "            signals.append(s_frag)\n",
        "            labels.append(l_frag)\n",
        "\n",
        "            # Go forth\n",
        "            sta += stride\n",
        "            end += stride\n",
        "\n",
        "    # Turn into arrays\n",
        "    signals = np.array(signals)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return signals, labels\n",
        "\n",
        "def create_datasets():\n",
        "    \"\"\" Training, validation, test \"\"\"\n",
        "    # Prepare paths\n",
        "    records = get_records()\n",
        "\n",
        "    # Shuffle up determinitically\n",
        "    np.random.seed(666)\n",
        "    np.random.shuffle(records)\n",
        "\n",
        "    # Define the data\n",
        "    width = 200\n",
        "\n",
        "    # Make training\n",
        "    make_dataset(records[:30], width, 'data/training')\n",
        "\n",
        "    # ... validation ...\n",
        "    make_dataset(records[30 : 39], width, 'data/validation')\n",
        "\n",
        "    # ... and test\n",
        "    make_dataset(records[39 : 48], width, 'data/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL6YmdIXH9Hw"
      },
      "source": [
        "Testing the Stress Image Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSf97hWxH1JP"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uXUbJbH9iIN"
      },
      "source": [
        "pip install tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruac9XU3Ikv-"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5Br8dhHvFng"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJtQSUffYc68"
      },
      "source": [
        "from os import listdir\n",
        "from xml.etree import ElementTree\n",
        "from numpy import zeros\n",
        "from numpy import asarray\n",
        "\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.model import MaskRCNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1PU1CTiexTT"
      },
      "source": [
        "pip install mrcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suBeoTTkrme9"
      },
      "source": [
        "import tensorflow \n",
        "# from adamod import AdaMod\n",
        "#loading the training images\n",
        "\n",
        "x_train = pd.read_csv('/content/drive/MyDrive/Stress_image_detection/data/train.csv')\n",
        "x_train_img = x_train['filename']\n",
        "\n",
        "\n",
        "# # load the image for google drive file to train_img\n",
        "train_image = []\n",
        "for i in tqdm(x_train_img):\n",
        "    img = image.load_img('/content/drive/MyDrive/Stress_image_detection/Stress_image_dataset/train/' + str(i), target_size=(128,128,3), grayscale = False)\n",
        "    img = image.img_to_array(img)\n",
        "    # img = tf.image.rgb_to_grayscale(img)\n",
        "    # img = np.expand_dims(img, axis=0)\n",
        "    img = img/255\n",
        "    train_image.append(img)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljLG5RgiX4BY"
      },
      "source": [
        "pip install adamod"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoZvz3R0WWY4"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class AdaMod(Optimizer):\n",
        "    \"\"\"Implements AdaMod algorithm with Decoupled Weight Decay (arxiv.org/abs/1711.05101)\n",
        "    It has been proposed in `Adaptive and Momental Bounds for Adaptive Learning Rate Methods`_.\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        beta3 (float, optional): smoothing coefficient for adaptive learning rates (default: 0.9999)\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), beta3=0.999,\n",
        "                 eps=1e-8, weight_decay=0):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= beta3 < 1.0:\n",
        "            raise ValueError(\"Invalid beta3 parameter: {}\".format(beta3))\n",
        "        defaults = dict(lr=lr, betas=betas, beta3=beta3, eps=eps,\n",
        "                        weight_decay=weight_decay)\n",
        "        super(AdaMod, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(AdaMod, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\n",
        "                        'AdaMod does not support sparse gradients')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of actual learning rates\n",
        "                    state['exp_avg_lr'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq, exp_avg_lr = state['exp_avg'], state['exp_avg_sq'], state['exp_avg_lr']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "\n",
        "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p.data.add_(-group['weight_decay'] * group['lr'], p.data)\n",
        "\n",
        "                # Applies momental bounds on actual learning rates\n",
        "                step_size = torch.full_like(denom, step_size)\n",
        "                step_size.div_(denom)\n",
        "                exp_avg_lr.mul_(group['beta3']).add_(1 - group['beta3'], step_size)\n",
        "                step_size = torch.min(step_size,  exp_avg_lr)\n",
        "                step_size.mul_(exp_avg)\n",
        "\n",
        "                p.data.add_(-step_size)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un5nDKZFz-ig"
      },
      "source": [
        "# # converting the images to an array\n",
        "X_train_img = np.array(train_image)\n",
        "\n",
        "# initializing the label\n",
        "# converting the text label to numbers\n",
        "y_train = []\n",
        "for i in x_train['class']:\n",
        "  if i == 'not_stressed':\n",
        "    i = 0\n",
        "  else:\n",
        "    i = 1\n",
        "    \n",
        "  y_train.append(i)\n",
        "\n",
        "# converting the label to categorical\n",
        "y_train_label = to_categorical(y_train)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dvy5FaZU_Fq"
      },
      "source": [
        "# Train image data augmentation\n",
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# train_aug\n",
        "\n",
        "train_aug = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    rotation_range = 20,\n",
        "    zoom_range = 0.15,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.15,\n",
        "    horizontal_flip = True,\n",
        "    fill_mode = \"nearest\"\n",
        "\n",
        ")\n",
        "\n",
        "# val_aug\n",
        "val_aug = ImageDataGenerator(\n",
        "    rotation_range = 20,\n",
        "    zoom_range = 0.15,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.15,\n",
        "    horizontal_flip = True,\n",
        "    fill_mode = \"nearest\"\n",
        "    \n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-MtHNGPPviu"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = train_datagen.\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    stress_train_dir,\n",
        "    target_size = (128, 128),\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    stress_val_dir,\n",
        "    target_size = (128, 128),\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary')\n",
        "\n",
        "\n",
        "\n",
        "for data_batch, labels_batch in train_generator:\n",
        "  print('data batch shape:', data_batch.shape)\n",
        "  print('labels batch shape', labels_batch.shape)\n",
        "  break\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch = 100,\n",
        "                              epochs = 30,\n",
        "                              validation_data = validation_generator,\n",
        "                              validation_steps = 50)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label = 'Trainning acc')\n",
        "plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
        "plt.title('Trainning and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label = 'Trainning loss')\n",
        "plt.plot(epochs, val_acc, 'b', label = 'Validation loss')\n",
        "plt.title('Trainning and validation loss')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpI8G52E4NfG"
      },
      "source": [
        "# loading validation data and converting to an array\n",
        "\n",
        "x_val = pd.read_csv('/content/drive/MyDrive/Stress_image_detection/data/valid.csv')\n",
        "x_val_img = x_val['filename']\n",
        "\n",
        "val_images = []\n",
        "\n",
        "for i in tqdm(x_val_img):\n",
        "  val_img = image.load_img('/content/drive/MyDrive/Stress_image_detection/Stress_image_dataset/valid/' + str(i), target_size=(128, 128, 3), grayscale = False)\n",
        "  val_img = image.img_to_array(val_img)\n",
        "  # val_img = tf.image.rgb_to_grayscale(val_img)\n",
        "  # val_img = np.expand_dims(val_img, axis=0)\n",
        "  val_img = val_img/255\n",
        "  val_images.append(val_img)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCoUTgRN55ml"
      },
      "source": [
        "# converting image to an array\n",
        "X_val = np.array(val_images)\n",
        "\n",
        "# initializing labl\n",
        "y_val = []\n",
        "\n",
        "for i in (x_val['class']):\n",
        "  if i == 'not_stressed':\n",
        "    i = 0\n",
        "  else:\n",
        "    i = 1\n",
        "  y_val.append(i)\n",
        "\n",
        "y_val_label = to_categorical(y_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arzpqluGgQrN"
      },
      "source": [
        "# loading validation data and converting to an array\n",
        "\n",
        "\n",
        "x_test = pd.read_csv('/content/drive/MyDrive/Stress_image_detection/data/test.csv')\n",
        "x_test_img = x_test['filename']\n",
        "\n",
        "test_images = []\n",
        "\n",
        "for i in tqdm(x_test_img):\n",
        "  test_img = image.load_img('/content/drive/MyDrive/Stress_image_detection/Stress_image_dataset/test/' + str(i), target_size=(128, 128, 3), grayscale=False)\n",
        "  test_img = image.img_to_array(test_img)\n",
        "  # test_img = tf.image.rgb_to_grayscale(test_img)\n",
        "  # test_img = np.expand_dims(test_img, axis=0)\n",
        "  test_img = test_img/255\n",
        "  test_images.append(test_img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT8Ww7digke_"
      },
      "source": [
        "X_test = np.array(x_test)\n",
        "\n",
        "y_test = []\n",
        "\n",
        "for i in (x_test['class']):\n",
        "  if i == 'not_stressed':\n",
        "    i = 0\n",
        "  else:\n",
        "    i = 1\n",
        "  y_test.append(i)\n",
        "\n",
        "y_test_label = to_categorical(y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNasq8WThSKB"
      },
      "source": [
        "# Training and Evaluating the data\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "# model = models.Sequential()\n",
        "# model.add(layers.Conv2D(32, kernel_size=(3,3), activation = 'relu', input_shape = (128, 128, 3)))\n",
        "# model.add(layers.MaxPooling2D((2,2)))\n",
        "# model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
        "# model.add(layers.MaxPooling2D((2,2)))\n",
        "# model.add(layers.Conv2D(128, (3,3), activation = 'relu'))\n",
        "# model.add(layers.MaxPooling2D((2,2)))\n",
        "# model.add(layers.Flatten())\n",
        "# model.add(layers.Dense(512, activation = 'relu'))\n",
        "# model.add(layers.Dense(2, activation = 'sigmoid'))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(1,1,128,128,1)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# model.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr = 1e-4), metrics = ['acc'])\n",
        "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['acc'])\n",
        "\n",
        "\n",
        "history = model.fit(train_aug.flow(X_train_img, y_train_label, batch_size= 8),\n",
        "                    steps_per_epoch = len(X_train_img)//8 ,\n",
        "                    epochs = 50,\n",
        "                    validation_data = (X_val, y_val_label)\n",
        "                   )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label = 'Trainning acc')\n",
        "plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
        "plt.title('Trainning and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label = 'Trainning loss')\n",
        "plt.plot(epochs, val_acc, 'b', label = 'Validation loss')\n",
        "plt.title('Trainning and validation loss')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anZbTj75aWH2"
      },
      "source": [
        "pip install keras-rectified-adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DD9rI56a_Os"
      },
      "source": [
        "pip install keras-adamw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qgWJnQYcjiN"
      },
      "source": [
        "pip install keras-gradient-accumulation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Ui3BdUcvak"
      },
      "source": [
        "pip install keras-adabound"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoBdcCX73RAV"
      },
      "source": [
        "X_train_new_denoised = np.array(train_denoised)\n",
        "# initializing the label\n",
        "# converting the text label to numbers\n",
        "y_train_denoised = []\n",
        "for i in x_train['class']:\n",
        "  if i == 'not_stressed':\n",
        "    i = 0\n",
        "  else:\n",
        "    i = 1\n",
        "    \n",
        "  y_train_denoised.append(i)\n",
        "\n",
        "# converting the label to categorical\n",
        "y_train_denoised_label = to_categorical(y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNdMa3Jx3-8J"
      },
      "source": [
        "X_val_new_denoised = np.array(val_denoised)\n",
        "# initializing the label\n",
        "# converting the text label to numbers\n",
        "\n",
        "\n",
        "# converting the label to categorical\n",
        "y_val_denoised_label = to_categorical(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRHjC1gR4WcU"
      },
      "source": [
        "X_train_aug = np.add(X_train_img, X_train_new_denoised)\n",
        "y_train_label_aug = np.add(y_train_label, y_train_denoised_label)\n",
        "\n",
        "X_val_aug = np.add(X_val, X_val_new_denoised)\n",
        "y_val_label_aug = np.add(y_val_label, y_val_denoised_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYiWo83rchT8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZtqttWmuI9y"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras_adabound import AdaBound\n",
        "# from keras_radam import RAdam\n",
        "\n",
        "# Using pre-trained VGG to classify stress\n",
        "model = VGG16(include_top = False, pooling = 'avg', weights = 'imagenet', input_shape = (128, 128, 3))\n",
        "for layer in model.layers:\n",
        "  layer.trainable = True\n",
        "\n",
        "# add new classifier layers\n",
        "flat1 = Flatten()(model.layers[-1].output)\n",
        "class1 = Dense(128, activation = 'relu', kernel_initializer='he_uniform')(flat1)\n",
        "output = Dense(2, activation= 'softmax')(class1)\n",
        "\n",
        "\n",
        "# add new model\n",
        "model = Model(inputs=model.inputs, outputs = output)\n",
        "# compile model\n",
        "opt2 = SGD(lr = 0.0001, momentum = 0.9)\n",
        "opt = AdaBound(lr=0.0001, final_lr=0.001, gamma = 0.0001, weight_decay= 0.)\n",
        "# opt3 = RAdam(total_steps=10000, warmup_proportion=0.01, min_lr=0.0001)\n",
        "model.compile(optimizer = opt, loss = 'mse', metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "# # load model\n",
        "# model = VGG16(include_top=False, input_shape=(128, 128, 3))\n",
        "# \t# mark loaded layers as not trainable\n",
        "# for layer in model.layers:\n",
        "# \tlayer.trainable = False\n",
        "# \t# add new classifier layers\n",
        "# flat1 = Flatten()(model.layers[-1].output)\n",
        "# class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "# output = Dense(1, activation='sigmoid')(class1)\n",
        "# \t# define new model\n",
        "# model = Model(inputs=model.inputs, outputs=output)\n",
        "# \t# compile model\n",
        "# opt = SGD(lr=0.001, momentum=0.9)\n",
        "# model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsSgwOvzX69x"
      },
      "source": [
        "Inception_V3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWWMxiklu6I"
      },
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "# univariate cnn example\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Dense, Convolution1D, MaxPool1D, Flatten, Dropout\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "kfold = KFold(n_splits = 10, shuffle=True)\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "\n",
        "inputs = np.append(X_train_img, X_val, axis = 0)\n",
        "targets = np.append(y_train_label, y_val_label, axis = 0)\n",
        "\n",
        "# for train, test in kfold.split(inputs, targets):\n",
        "for train, test in loo.split(inputs):\n",
        "  X_train_img, X_val = inputs[train], inputs[test]\n",
        "  y_train_label, y_val_label = targets[train], targets[test]\n",
        "    \n",
        "  base_model = InceptionV3(input_shape = (128, 128, 3), include_top = False, weights = 'imagenet')\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  flat1 = Flatten()(base_model.output)\n",
        "  class1 = Dense(1024, activation='relu')(flat1)\n",
        "  class1 = Dropout(0.2)(class1)\n",
        "  output = Dense(2, activation='softmax')(class1)\n",
        "\n",
        "  inc_model = Model(inputs = base_model.inputs, outputs=output)\n",
        "\n",
        "  inc_model.compile(optimizer = opt, loss = 'mse', metrics = ['accuracy'])\n",
        "\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=8)]\n",
        "              \n",
        "  # history = inc_model.fit(X_train_img, y_train_label, steps_per_epoch = len(X_train_img)/12, callbacks = callbacks,\n",
        "  #                     validation_data = (X_val, y_val_label), validation_steps = len(X_val)/12, epochs = 50)\n",
        "  \n",
        "  history = inc_model.fit(X_train_img, y_train_label, steps_per_epoch = len(X_train_img)/12, callbacks = callbacks, epochs = 50)\n",
        "\n",
        "#   scores = inc_model.evaluate(X_val, y_val_label, verbose = 1)\n",
        "\n",
        "#   print(f'Score for fold {fold_no}: {inc_model.metrics_names[0]} of {scores[0]}; {inc_model.metrics_names[1]} of {scores[1]*100}%')\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "#   acc_per_fold.append(scores[1] * 100)\n",
        "#   loss_per_fold.append(scores[0])\n",
        "\n",
        "#   # Increase fold number\n",
        "#   fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "# # == Provide average scores ==\n",
        "# print('------------------------------------------------------------------------')\n",
        "# print('Score per fold')\n",
        "# for i in range(0, len(acc_per_fold)):\n",
        "#   print('------------------------------------------------------------------------')\n",
        "#   print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "# print('------------------------------------------------------------------------')\n",
        "# print('Average scores for all folds:')\n",
        "# print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "# print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "# print('------------------------------------------------------------------------')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEgvibOYaV1W"
      },
      "source": [
        "Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2xric4NaXe3"
      },
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "\n",
        "kfold = KFold(n_splits = 10, shuffle=True)\n",
        "loo = LeaveOneOut()\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "\n",
        "inputs = np.append(X_train_img, X_val, axis = 0)\n",
        "targets = np.append(y_train_label, y_val_label, axis = 0)\n",
        "\n",
        "# for train, test in kfold.split(inputs, targets):\n",
        "for train, test in loo.split(inputs):\n",
        "  X_train_img, X_val = inputs[train], inputs[test]\n",
        "  y_train_label, y_val_label = targets[train], targets[test]\n",
        "\n",
        "\n",
        "  res_model = ResNet50(input_shape=(128, 128,3), include_top=False, weights=\"imagenet\")\n",
        "  for layer in res_model.layers:\n",
        "      layer.trainable = True\n",
        "\n",
        "\n",
        "  res_model = Sequential()\n",
        "  res_model.add(ResNet50(include_top=False, weights='imagenet', pooling='max'))\n",
        "  res_model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "  res_model.compile(optimizer = opt, loss = 'mse', metrics = ['accuracy'])\n",
        "\n",
        "# callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
        "#              ModelCheckpoint(filepath='resnet_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=8)]\n",
        "             \n",
        "  # history = res_model.fit(X_train_img, y_train_label, steps_per_epoch = len(X_train_img)/12, callbacks = callbacks,\n",
        "  #                     validation_data = (X_val, y_val_label), validation_steps = len(X_val)/12, epochs = 50)\n",
        "  \n",
        "  history = res_model.fit(X_train_img, y_train_label, steps_per_epoch = len(X_train_img)/12, callbacks = callbacks, epochs = 50)\n",
        "  \n",
        "#   scores = model.evaluate(X_val, y_val_label, verbose = 1)\n",
        "\n",
        "#   print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "#   acc_per_fold.append(scores[1] * 100)\n",
        "#   loss_per_fold.append(scores[0])\n",
        "\n",
        "#   # Increase fold number\n",
        "#   fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "# # == Provide average scores ==\n",
        "# print('------------------------------------------------------------------------')\n",
        "# print('Score per fold')\n",
        "# for i in range(0, len(acc_per_fold)):\n",
        "#   print('------------------------------------------------------------------------')\n",
        "#   print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "# print('------------------------------------------------------------------------')\n",
        "# print('Average scores for all folds:')\n",
        "# print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "# print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "# print('------------------------------------------------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iESf-lwkbTGw"
      },
      "source": [
        "VGG19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCAxiZYibXP_"
      },
      "source": [
        "import os\n",
        "\n",
        "import random\n",
        "import warnings\n",
        "from sklearn.metrics import confusion_matrix\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#File Operation libraries\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "#Visualisation Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "#Keras\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, AvgPool2D, Input, Dropout, Flatten, BatchNormalization\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg19 import VGG19\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "\n",
        "kfold = KFold(n_splits = 10, shuffle=True)\n",
        "loo = LeaveOneOut()\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "\n",
        "inputs = np.append(X_train_img, X_val, axis = 0)\n",
        "targets = np.append(y_train_label, y_val_label, axis = 0)\n",
        "\n",
        "# for train, test in kfold.split(inputs, targets):\n",
        "for train, test in loo.split(inputs):\n",
        "  X_train_img, X_val = inputs[train], inputs[test]\n",
        "  y_train_label, y_val_label = targets[train], targets[test]\n",
        "\n",
        "  vgg19_model = VGG19(weights='imagenet', pooling = 'avg', include_top=False, input_shape = (128, 128, 3))\n",
        "  for layer in vgg19_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "\n",
        "  flat1 = Flatten()(vgg19_model.layers[-1].output)\n",
        "  class1 = Dense(128, activation = 'relu', kernel_initializer='he_uniform')(flat1)\n",
        "  output = Dense(2, activation= 'softmax')(class1)\n",
        "\n",
        "  model = Model(inputs=vgg19_model.inputs, outputs = output)\n",
        "\n",
        "  model.compile(optimizer = opt, loss = 'mse', metrics = ['accuracy'])\n",
        "\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=8)]\n",
        "              \n",
        "  # history = model.fit(X_train_img, y_train_label, steps_per_epoch = len(X_train_img)/12, callbacks = callbacks,\n",
        "  #                     validation_data = (X_val, y_val_label), validation_steps = len(X_val)/12, epochs = 50)\n",
        "  history = model.fit(X_train_img, y_train_label, steps_per_epoch = len(X_train_img)/12, callbacks = callbacks, epochs = 50)\n",
        "  \n",
        "  \n",
        "\n",
        "#   scores = model.evaluate(X_val, y_val_label, verbose = 1)\n",
        "\n",
        "#   print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "#   acc_per_fold.append(scores[1] * 100)\n",
        "#   loss_per_fold.append(scores[0])\n",
        "\n",
        "#   # Increase fold number\n",
        "#   fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "# # == Provide average scores ==\n",
        "# print('------------------------------------------------------------------------')\n",
        "# print('Score per fold')\n",
        "# for i in range(0, len(acc_per_fold)):\n",
        "#   print('------------------------------------------------------------------------')\n",
        "#   print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "# print('------------------------------------------------------------------------')\n",
        "# print('Average scores for all folds:')\n",
        "# print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "# print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "# print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CZP4SywukKQ"
      },
      "source": [
        "Xception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sejEh3cqCy16"
      },
      "source": [
        "# plotting the model curves\n",
        "plt.figure(figsize=(10, 10))\n",
        "acc = history5.history['accuracy']\n",
        "val_acc = history5.history['val_accuracy']\n",
        "loss = history5.history['loss']\n",
        "val_loss = history5.history['val_loss']\n",
        "\n",
        "\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('MSE Loss (Adadelta)')\n",
        "pyplot.plot(loss, color='blue', label='train loss')\n",
        "pyplot.plot(val_loss, color='orange', label='val loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show\n",
        "\t# plot accuracy\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Classification Accuracy using Adadelta')\n",
        "pyplot.plot(acc, color='blue', label='train acc')\n",
        "pyplot.plot(val_acc, color='orange', label='val acc')\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Acc')\n",
        "plt.legend()\n",
        "plt.show\n",
        "\t# save plot to file\n",
        "# filename = sys.argv[0].split('/')[-1]\n",
        "# pyplot.savefig(filename + '_plot.png')\n",
        "# pyplot.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTyJRQJrEcaA"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('MSE Loss of all the optimizers')\n",
        "plt.plot(history.history['val_loss'],color='blue', label='Adabound')\n",
        "plt.plot(history2.history['val_loss'],color='green', label='SGD')\n",
        "plt.plot(history3.history['val_loss'],color='orange', label='Adam')\n",
        "plt.plot(history4.history['val_loss'],color='Red', label='Rdam')\n",
        "plt.plot(history5.history['val_loss'],color='purple', label='Adadelta')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show\n",
        "\n",
        "\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy using of all the optimizers')\n",
        "plt.plot(history.history['val_accuracy'],color='blue', label='Adabound')\n",
        "plt.plot(history2.history['val_accuracy'],color='green', label='SGD')\n",
        "plt.plot(history3.history['val_accuracy'],color='orange', label='Adam')\n",
        "plt.plot(history4.history['val_accuracy'],color='Red', label='Rdam')\n",
        "plt.plot(history5.history['val_accuracy'],color='purple', label='Adadelta')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Acc')\n",
        "plt.legend()\n",
        "plt.show\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDG3L-gowXIr"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "\n",
        "import random\n",
        "import warnings\n",
        "from sklearn.metrics import confusion_matrix\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#File Operation libraries\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "#Visualisation Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "#Keras\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, AvgPool2D, Input, Dropout, Flatten, BatchNormalization\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg19 import VGG19\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "# running the model and getting results\n",
        "\n",
        "# datagen = ImageDataGenerator(feature_center = True)\n",
        "# # specify mean values for centering\n",
        "# datagen.mean = [123.68, 116.779, 103.939]\n",
        "# # prepare iterator\n",
        "# train_it = datagen.flow((X_train_img, y_train_label),\n",
        "#                         class_mode = 'binary',\n",
        "#                         batch_size = 30, target_size = (128, 128))\n",
        "# test_it = datagen.flow((X_val, y_val_label),\n",
        "#                        class_mode = 'binary',\n",
        "#                        batch_size = 30,\n",
        "#                        target_size = (128, 128))\n",
        "# \n",
        "\n",
        "# Adabound\n",
        "# history = model.fit(X_train_img, y_train_label, steps_per_epoch = len(X_train_img),\n",
        "#                     validation_data = (X_val, y_val_label), validation_steps = len(X_val), epochs = 50)\n",
        "\n",
        "# SGD\n",
        "\n",
        "kfold = KFold(n_splits = 10, shuffle=True)\n",
        "loo = LeaveOneOut()\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "\n",
        "inputs = np.append(X_train_img, X_val, axis = 0)\n",
        "targets = np.append(y_train_label, y_val_label, axis = 0)\n",
        "\n",
        "# for train, test in kfold.split(inputs, targets):\n",
        "for train, test in loo.split(inputs):\n",
        "  X_train_img, X_val = inputs[train], inputs[test]\n",
        "  y_train_label, y_val_label = targets[train], targets[test]\n",
        "\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=8)]\n",
        "              \n",
        "  # history = model.fit(X_train_img, y_train_label, steps_per_epoch = len(X_train_img)/12, callbacks = callbacks,\n",
        "  #                     validation_data = (X_val, y_val_label), validation_steps = len(X_val)/12, epochs = 50)\n",
        "  \n",
        "  history = model.fit(X_train_img, y_train_label, steps_per_epoch = len(X_train_img)/12, callbacks = callbacks, epochs = 50)\n",
        "\n",
        "\n",
        "#   scores = model.evaluate(X_val, y_val_label, verbose = 1)\n",
        "\n",
        "#   print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "#   acc_per_fold.append(scores[1] * 100)\n",
        "#   loss_per_fold.append(scores[0])\n",
        "\n",
        "#   # Increase fold number\n",
        "#   # fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "# # == Provide average scores ==\n",
        "# print('------------------------------------------------------------------------')\n",
        "# print('Score per fold')\n",
        "# for i in range(0, len(acc_per_fold)):\n",
        "#   print('------------------------------------------------------------------------')\n",
        "#   print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "# print('------------------------------------------------------------------------')\n",
        "# print('Average scores for all folds:')\n",
        "# print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "# print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "# print('------------------------------------------------------------------------')\n",
        "\n",
        "# # preds = model.predict(X_test)\n",
        "# pred_class = np.argmax(preds, axis = 1)\n",
        "\n",
        "# acc = model.accuracy_score(y_test_label, pred_class, verbose = 0)\n",
        "# print(\"VGG16 Model Accuracy without Fine-Tuning: {:.2f}%\".format(acc * 100))\n",
        "# # learning curves\n",
        "# summarize_diagnostics(history) \n",
        "\n",
        "\n",
        "\n",
        "# datagen = ImageDataGenerator(featurewise_center=True)\n",
        "# \t# specify imagenet mean values for centering\n",
        "# \tdatagen.mean = [123.68, 116.779, 103.939]\n",
        "# \t# prepare iterator\n",
        "# \ttrain_it = datagen.flow_from_directory('dataset_dogs_vs_cats/train/',\n",
        "# \t\tclass_mode='binary', batch_size=64, target_size=(224, 224))\n",
        "# \ttest_it = datagen.flow_from_directory('dataset_dogs_vs_cats/test/',\n",
        "# \t\tclass_mode='binary', batch_size=64, target_size=(224, 224))\n",
        "# \t# fit model\n",
        "# \thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
        "# \t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=10, verbose=1)\n",
        "# \t# evaluate model\n",
        "# \t_, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
        "# \tprint('> %.3f' % (acc * 100.0))\n",
        "# \t# learning curves\n",
        "# \tsummarize_diagnostics(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sinlp79BoCfW"
      },
      "source": [
        "Genetic Algortihm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufxjS8IGoERA"
      },
      "source": [
        "class CNN(Sequential):\n",
        "    def __init__(self,nfilters,sfilters):\n",
        "        super().__init__()\n",
        "        tensorflow.random.set_seed(0)\n",
        "        self.vgg = VGG16vgg = VGG16(include_top = False, pooling = 'avg', weights = 'imagenet', input_shape = (128, 128, 3))\n",
        "        for layer in model.layers:\n",
        "          layer.trainable = False \n",
        "        self.layer_name = 'block1_pool'\n",
        "        self.my_model = Model(inputs= self.vgg.inputs, outputs = self.vgg.get_layer(self.layer_name).output)\n",
        "        self.add(Conv2D(nfilters[0],kernel_size=(sfilters[0],sfilters[0]),padding='same',activation='relu'))\n",
        "        self.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "        self.add(Conv2D(nfilters[1],kernel_size=(sfilters[1],sfilters[1]),padding='same',activation='relu'))\n",
        "        self.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "        self.add(Conv2D(nfilters[2],kernel_size=(sfilters[2],sfilters[2]),padding='same',activation='relu'))\n",
        "        self.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "        self.add(Flatten())\n",
        "        self.add(Dropout(0.3))\n",
        "        self.add(Dense(512,activation='relu'))\n",
        "        self.add(Dense(2,activation='softmax'))\n",
        "        self.opt = AdaBound(lr=0.0001, final_lr=0.01, gamma = 0.0001, weight_decay= 0.)\n",
        "        self.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLwDqY35oN4Q"
      },
      "source": [
        "class Genetic:\n",
        "    \n",
        "    def __init__(self,pop_size,nlayers,max_nfilters,max_sfilters):\n",
        "        self.pop_size = pop_size\n",
        "        self.nlayers = nlayers\n",
        "        self.max_nfilters = max_nfilters\n",
        "        self.max_sfilters = max_sfilters\n",
        "        self.max_acc = 0\n",
        "        self.best_arch = np.zeros((1,6))\n",
        "        self.gen_acc = []\n",
        "    \n",
        "    def generate_population(self):\n",
        "        np.random.seed(0)\n",
        "        pop_nlayers = np.random.randint(1,self.max_nfilters,(self.pop_size,self.nlayers))\n",
        "        pop_sfilters = np.random.randint(1,self.max_sfilters,(self.pop_size,self.nlayers))\n",
        "        pop_total = np.concatenate((pop_nlayers,pop_sfilters),axis=1)\n",
        "        return pop_total\n",
        "    \n",
        "    def select_parents(self,pop,nparents,fitness):\n",
        "        parents = np.zeros((nparents,pop.shape[1]))\n",
        "        for i in range(nparents):\n",
        "            best = np.argmax(fitness)\n",
        "            parents[i] = pop[best]\n",
        "            fitness[best] = -99999\n",
        "        return parents\n",
        "    \n",
        "    def crossover(self,parents):\n",
        "        nchild = self.pop_size - parents.shape[0]\n",
        "        nparents = parents.shape[0]\n",
        "        child = np.zeros((nchild,parents.shape[1]))\n",
        "        for i in range(nchild):\n",
        "            first = i % nparents\n",
        "            second = (i+1) % nparents\n",
        "            child[i,:2] = parents[first][:2]\n",
        "            child[i,2] = parents[second][2]\n",
        "            child[i,3:5] = parents[first][3:5]\n",
        "            child[i,5] = parents[second][5]\n",
        "        return child\n",
        "\n",
        "    def mutation(self,child):\n",
        "        for i in range(child.shape[0]):\n",
        "            val = np.random.randint(1,6)\n",
        "            ind = np.random.randint(1,4) - 1\n",
        "            if child[i][ind] + val > 100:\n",
        "                child[i][ind] -= val\n",
        "            else:\n",
        "                child[i][ind] += val\n",
        "            val = np.random.randint(1,4)\n",
        "            ind = np.random.randint(4,7) - 1\n",
        "            if child[i][ind] + val > 20:\n",
        "                child[i][ind] -= val\n",
        "            else:\n",
        "                child[i][ind] += val\n",
        "        return child\n",
        "    \n",
        "    def fitness(self,pop,X,Y,epochs):\n",
        "        pop_acc = []\n",
        "        for i in range(pop.shape[0]):\n",
        "            nfilters = pop[i][0:3]\n",
        "            sfilters = pop[i][3:]\n",
        "            model = CNN(nfilters,sfilters)\n",
        "            H = model.fit(X,Y,batch_size=32,epochs=epochs)\n",
        "            acc = H.history['accuracy']\n",
        "            pop_acc.append(max(acc)*100)\n",
        "        if max(pop_acc) > self.max_acc:\n",
        "            self.max_acc = max(pop_acc)\n",
        "            self.best_arch = pop[np.argmax(pop_acc)]\n",
        "        self.gen_acc.append(max(pop_acc))\n",
        "        return pop_acc\n",
        "    \n",
        "    def smooth_curve(self,factor,gen):\n",
        "        smoothed_points = []\n",
        "        for point in self.gen_acc:\n",
        "            if smoothed_points:\n",
        "                prev = smoothed_points[-1]\n",
        "                smoothed_points.append(prev*factor + point * (1-factor))\n",
        "            else:\n",
        "                smoothed_points.append(point)\n",
        "        plt.plot(range(gen+1),smoothed_points,'g',label='Smoothed training acc')\n",
        "        plt.xticks(np.arange(gen+1))\n",
        "        plt.legend()\n",
        "        plt.title('Fitness Accuracy vs Generations')\n",
        "        plt.xlabel('Generations')\n",
        "        plt.ylabel('Fitness (%)')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3iETiQaoPFr"
      },
      "source": [
        "pop_size = 10\n",
        "nlayers = 3\n",
        "max_nfilters = 100\n",
        "max_sfilters = 20\n",
        "epochs = 20\n",
        "num_generations = 3\n",
        "\n",
        "genCNN = Genetic(pop_size,nlayers,max_nfilters,max_sfilters)\n",
        "pop = genCNN.generate_population()\n",
        "\n",
        "for i in range(num_generations+1):\n",
        "    pop_acc = genCNN.fitness(pop,X_train_img,y_train_label,epochs)\n",
        "    print('Best Accuracy at the generation {}: {}'.format(i,genCNN.max_acc))\n",
        "    parents = genCNN.select_parents(pop,5,pop_acc.copy())\n",
        "    child = genCNN.crossover(parents)\n",
        "    child = genCNN.mutation(child)\n",
        "    pop = np.concatenate((parents,child),axis=0).astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TlchTDy3SGF"
      },
      "source": [
        "genCNN.gen_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMs-wesk3Kkz"
      },
      "source": [
        "# Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdziWSNo3IGP"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from keras.layers import Conv2D, Input, Dense, Dropout, MaxPool2D, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist, cifar10\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuKfUIkryaJ3"
      },
      "source": [
        "# Convolutional Autoencoder\n",
        "\n",
        "from keras.layers import Conv2DTranspose, BatchNormalization, add, LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import SGD\n",
        "from keras_adabound import AdaBound"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6aGM56ERuz0"
      },
      "source": [
        "# Adding noise mean = 0, std = 0.3\n",
        "noise = 0.3\n",
        "train_noise = X_train_img + noise * np.random.normal(0, 0.3, size= X_train_img.shape) \n",
        "val_noise = X_val + noise * np.random.normal(0, 0.3, size= X_val.shape)\n",
        "\n",
        "train_noise = np.clip(train_noise, 0, 1)\n",
        "val_noise = np.clip(val_noise, 0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y02gn1pCSOum"
      },
      "source": [
        "rows = 2 # defining no. of rows in figure\n",
        "cols = 8 # defining no. of colums in figure\n",
        "\n",
        "f = plt.figure(figsize=(2*cols,2*rows*2)) # defining a figure \n",
        "\n",
        "for i in range(rows):\n",
        "    for j in range(cols): \n",
        "        f.add_subplot(rows*2,cols, (2*i*cols)+(j+1)) # adding sub plot to figure on each iteration\n",
        "        plt.imshow(train_noise[i*cols + j]) \n",
        "        plt.axis(\"off\")\n",
        "        \n",
        "    for j in range(cols): \n",
        "        f.add_subplot(rows*2,cols,((2*i+1)*cols)+(j+1)) # adding sub plot to figure on each iteration\n",
        "        plt.imshow(X_train_img[i*cols + j]) \n",
        "        plt.axis(\"off\")\n",
        "        \n",
        "f.suptitle(\"Sample Training Data\",fontsize=18)\n",
        "plt.savefig(\"Cifar-trian.png\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjv3cwss6QBl"
      },
      "source": [
        "# Encoder\n",
        "inputs = Input(shape = (128, 128, 3))\n",
        "\n",
        "x = Conv2D(128, 3, activation='relu', padding='same')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "skip = Conv2D(128, 3, padding='same')(x) # skip connection for decoder\n",
        "x = LeakyReLU()(skip)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Conv2D(256, 3, activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "encoded = MaxPool2D()(x)\n",
        "\n",
        "# Decoder\n",
        "x = Conv2DTranspose(256, 3,activation='relu',strides=(2,2), padding='same')(encoded)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Conv2DTranspose(128, 3, activation='relu',strides=(2,2), padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Conv2DTranspose(128, 3, padding='same')(x)\n",
        "x = add([x,skip]) # adding skip connection\n",
        "x = LeakyReLU()(x)\n",
        "x = BatchNormalization()(x)\n",
        "decoded = Conv2DTranspose(3, 3, activation='sigmoid',strides=(2,2), padding='same')(x)\n",
        "\n",
        "autoencoder = Model(inputs, decoded)\n",
        "opt = AdaBound(lr=0.001, final_lr=0.1, gamma = 0.0001, weight_decay= 0.)\n",
        "# opt = SGD(lr = 0.0001, momentum = 0.1)\n",
        "autoencoder.compile(optimizer=opt, loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cm9hWA0VXBu"
      },
      "source": [
        "# Training\n",
        "epochs = 50\n",
        "batch_size = 256\n",
        "\n",
        "history = autoencoder.fit(train_noise,\n",
        "                X_train_img,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(val_noise, X_val)\n",
        "               )\n",
        "\n",
        "\n",
        "# Defining Figure\n",
        "f = plt.figure(figsize=(10,7))\n",
        "f.add_subplot()\n",
        "\n",
        "#Adding Subplot\n",
        "plt.plot(history.epoch, history.history['loss'], label = \"loss\") # Loss curve for training set\n",
        "plt.plot(history.epoch, history.history['val_loss'], label = \"val_loss\") # Loss curve for validation set\n",
        "\n",
        "plt.title(\"Loss Curve\",fontsize=18)\n",
        "plt.xlabel(\"Epochs\",fontsize=15)\n",
        "plt.ylabel(\"Loss\",fontsize=15)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.savefig(\"Loss_curve_stress.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu67dBh9cUPg"
      },
      "source": [
        "train_denoised = autoencoder.predict(X_train_img)\n",
        "val_denoised =  autoencoder.predict(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6_EbAwWcj9s"
      },
      "source": [
        "# Visualize test images with their denoised images\n",
        "\n",
        "rows = 4 # defining no. of rows in figure\n",
        "cols = 12 # defining no. of colums in figure\n",
        "cell_size = 1.5\n",
        "f = plt.figure(figsize=(cell_size*cols,cell_size*rows*2)) # defining a figure \n",
        "f.tight_layout()\n",
        "for i in range(rows):\n",
        "    for j in range(cols): \n",
        "        f.add_subplot(rows*2,cols, (2*i*cols)+(j+1)) # adding sub plot to figure on each iteration\n",
        "        plt.imshow(X_train_vae_new[i*cols + j]) \n",
        "        plt.axis(\"off\")\n",
        "        \n",
        "    for j in range(cols): \n",
        "        f.add_subplot(rows*2,cols,((2*i+1)*cols)+(j+1)) # adding sub plot to figure on each iteration\n",
        "        plt.imshow(X_val_vae_new[i*cols + j]) \n",
        "        plt.axis(\"off\")\n",
        "\n",
        "f.suptitle(\"Autoencoder Results - Adabound (lr = 0.001, final lr = 0.1)\",fontsize=18)\n",
        "plt.savefig(\"Adabound Autoencoder.png\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJkUO7YW-2PX"
      },
      "source": [
        "Autoencoder balance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKfcoVqH-5JX"
      },
      "source": [
        "from keras.layers import Input, Dense, Lambda, Reshape\n",
        "from keras.models import Model, Sequential\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import preprocessing \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "np.random.seed(203)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F53Yg83F_N1x"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as kr\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6EugjoeA2KE"
      },
      "source": [
        "# Dataset parameters.\n",
        "num_features = 49152 # data features (img shape: 28*28).\n",
        "\n",
        "# Training parameters.\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "filters = 128\n",
        "\n",
        "# Network Parameters\n",
        "hidden_1 = 128 # 1st layer num features.\n",
        "hidden_2 = 64 # 2nd layer num features (the latent dim).  \n",
        "\n",
        "\n",
        "\n",
        "# X_train and X_val reshaped for VAE\n",
        "X_train_vae = np.concatenate([X_train_img, X_val])\n",
        "X_train_vae = X_train_vae.astype('float32')/255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Reshape train and val\n",
        "X_train_reshape = X_train_img.reshape([-1, 128, 128, 3])\n",
        "X_val_reshape = X_val.reshape([-1, 128, 128, 3])\n",
        "\n",
        "X_train_reshape = X_train_reshape.astype(np.float32, copy=False)\n",
        "X_val_reshape = X_val_reshape.astype(np.float32, copy=False)\n",
        "\n",
        "\n",
        "# x_train_re = X_train_img.astype('float32') / 255.\n",
        "# x_val_re = X_val.astype('float32') / 255.\n",
        "# x_train_re = X_train_img.reshape((len(X_train_img), np.prod(X_train_img.shape[1:])))\n",
        "# x_val_re = X_val.reshape((len(X_val), np.prod(X_val.shape[1:])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQw7ttR1AbTR"
      },
      "source": [
        "# plot predictions\n",
        "\n",
        "def plot_predictions(y_true, y_pred):    \n",
        "    f, ax = plt.subplots(2, 10, figsize=(15, 4))\n",
        "    for i in range(10):\n",
        "        ax[0][i].imshow(np.reshape(y_true[i], (128, 128, 3)), aspect='auto')\n",
        "        ax[1][i].imshow(np.reshape(y_pred[i], (128, 128, 3)), aspect='auto')\n",
        "    plt.tight_layout()\n",
        "\n",
        "# plot digits\n",
        "\n",
        "\n",
        "\n",
        "def plot_digits(X, y, encoder, batch_size=128):\n",
        "    \"\"\"Plots labels and MNIST digits as function of 2D latent vector\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    encoder: Model\n",
        "        A Keras Model instance\n",
        "    X: np.ndarray\n",
        "        Test data\n",
        "    y: np.ndarray\n",
        "        Test data labels\n",
        "    batch_size: int\n",
        "        Prediction batch size\n",
        "    \"\"\"\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, _, _ = encoder.predict(X, batch_size=batch_size)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0] Latent Dimension\")\n",
        "    plt.ylabel(\"z[1] Latent Dimension\")\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "# Plot manfold\n",
        "\n",
        "def generate_manifold(decoder):\n",
        "    \"\"\"Generates a manifold of MNIST digits from a random noisy data.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    decoder: Model\n",
        "        A Keras Model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    # display a 30x30 2D manifold of digits\n",
        "    n = 30\n",
        "    digit_size = 28\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    \n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    grid_x = np.linspace(-4, 4, n)\n",
        "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
        "    \n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[i * digit_size: (i + 1) * digit_size,\n",
        "                   j * digit_size: (j + 1) * digit_size] = digit        \n",
        "    \n",
        "    plt.figure(figsize=(10, 10))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = n * digit_size + start_range + 1\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    \n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0] Latent Dimension\")\n",
        "    plt.ylabel(\"z[1] Latent Dimension\")\n",
        "    plt.imshow(figure, cmap='Greys_r')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3SJ3QUpA93C"
      },
      "source": [
        "# Variaional Encoder\n",
        "\n",
        "inputs = kr.Input(shape=(num_features, ))\n",
        "encoder = kr.layers.Dense(hidden_1, activation='sigmoid')(inputs)\n",
        "encoder = kr.layers.Dense(hidden_2, activation='sigmoid')(encoder)\n",
        "encoder_model = kr.Model(inputs, encoder, name='encoder')\n",
        "encoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYIA1o5lBRri"
      },
      "source": [
        "# Decoder\n",
        "\n",
        "latent_dim = kr.Input(shape=(hidden_2, ))\n",
        "decoder = kr.layers.Dense(hidden_1, activation='sigmoid')(latent_dim)\n",
        "decoder = kr.layers.Dense(num_features, activation='sigmoid')(decoder)\n",
        "decoder_model = kr.Model(latent_dim, decoder, name='decoder')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61e_Q-VnBhp6"
      },
      "source": [
        "# Autoencoder model\n",
        "\n",
        "outputs = decoder_model(encoder_model(inputs))\n",
        "stress_model = kr.Model(inputs, outputs )\n",
        "stress_model.compile(optimizer='adam', loss='mse')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJM8PjX2Bzyo"
      },
      "source": [
        "# trainingg\n",
        "stress_model.fit(x=X_train_reshape, y=X_train_reshape, batch_size=batch_size, shuffle=False, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQLOhYOYDBRp"
      },
      "source": [
        "\n",
        "y_pred = stress_model.predict(X_val_reshape)\n",
        "plot_predictions(X_val, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0oYh2oEEYbS"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "class Sampling(Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP1wz-fBEeOv"
      },
      "source": [
        "# new autoencoder\n",
        "\n",
        "original_dim = 128 * 128 * 3\n",
        "input_shape = (128, 128, 3)\n",
        "hidden_dim = 512\n",
        "\n",
        "# The bigger this is, more accurate the network is but 2 is for illustration purposes.\n",
        "latent_dim = 10\n",
        " \n",
        "\n",
        "# encoder\n",
        "\n",
        "encoder_inputs = Input(shape=(128, 128, 3))\n",
        "x = Conv2D(128, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = Conv2D(256, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(64, activation=\"relu\")(x)\n",
        "z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()\n",
        "\n",
        "# implement the decoder\n",
        "\n",
        "latent_inputs = Input(shape=(latent_dim,))\n",
        "x = Dense(32 * 32 * 256, activation=\"relu\")(latent_inputs)\n",
        "x = Reshape((32, 32, 256))(x)\n",
        "x = Conv2DTranspose(256, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = Conv2DTranspose(128, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()\n",
        "\n",
        "\n",
        "# VAE implementation\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(Adam())\n",
        "vae.fit(X_train_vae, epochs=50, batch_size=128)\n",
        "\n",
        "\n",
        "# vae = Model(inputs, outputs, name='vae')\n",
        "# # reconstruction loss\n",
        "# reconstruction_loss = keras.losses.binary_crossentropy(inputs, outputs)\n",
        "# reconstruction_loss *= (original_dim)\n",
        "\n",
        "\n",
        "# kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "\n",
        "# kl_loss = K.sum(kl_loss, axis=-1)\n",
        "# kl_loss *= -0.5\n",
        "# vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "# vae.add_loss(vae_loss)\n",
        "\n",
        "\n",
        "# vae.add_loss(vae_loss)\n",
        "\n",
        "vae.compile(optimizer='adam')\n",
        "# # print(outputs.shape)\n",
        "\n",
        "# vae.fit(X_train_reshape, X_train_reshape, epochs=epochs, batch_size=batch_size, validation_data=(X_val_reshape, X_val_reshape))\n",
        "\n",
        "# inputs = kr.layers.Input(shape=(num_features, ), name='input')\n",
        "# x = kr.layers.Dense(hidden_dim, activation='relu')(inputs)\n",
        "# z_mean = kr.layers.Dense(latent_dim, name='z_mean')(x)\n",
        "# z_log_var = kr.layers.Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "# # reparamaterization\n",
        "# z = kr.layers.Lambda(sampling, name='z')([z_mean, z_log_var])\n",
        "# # instantiate encoder model\n",
        "# encoder = kr.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "\n",
        "# # Decoder\n",
        "# latent_inputs = kr.layers.Input(shape=(latent_dim,), name='z_sampling')\n",
        "# x = kr.layers.Dense(hidden_dim, activation='relu')(latent_inputs)\n",
        "# outputs = kr.layers.Dense(num_features, activation='sigmoid')(x)\n",
        "\n",
        "# # instantiate decoder model\n",
        "# decoder = kr.Model(latent_inputs, outputs, name='decoder')\n",
        "\n",
        "# # # VAE model = encoder + decoder\n",
        "# outputs = decoder(encoder(inputs)[2])  # Select the Z value from outputs of the encoder\n",
        "# vae = kr.Model(inputs, outputs, name='vae')\n",
        "\n",
        "\n",
        "# # Reconstruction loss\n",
        "# reconstruction_loss = tf.losses.mean_squared_error(inputs, outputs)\n",
        "# reconstruction_loss = reconstruction_loss * num_features\n",
        "\n",
        "# # KL Divergence loss\n",
        "# kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "# kl_loss = -0.5 * tf.reduce_sum(kl_loss, axis=-1)\n",
        "# vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "# vae.add_loss(vae_loss)\n",
        "# vae.compile(optimizer='adam')\n",
        "\n",
        "# vae.fit(X_train_reshape, epochs=epochs, batch_size=batch_size, validation_data=(X_val_reshape, None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbj4GhJaAvWN"
      },
      "source": [
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-zAzlcZ6EmA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_latent_space(vae, n=30, figsize=15):\n",
        "    # display a n*n 2D manifold of digits\n",
        "    digit_size = 128\n",
        "    scale = 1.0\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    grid_x = np.linspace(-scale, scale, n)\n",
        "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = vae.decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size, 3)\n",
        "            figure[\n",
        "                i * digit_size : (i + 1) * digit_size, \n",
        "                j * digit_size : (j + 1) * digit_size, \n",
        "            ] = digit\n",
        "\n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = n * digit_size + start_range\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap=\"Greys_r\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_latent_space(vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTiPda_c65IQ"
      },
      "source": [
        "def plot_label_clusters(vae, data, labels):\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, _, _ = vae.encoder.predict(data)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "X_train_re = X_train_img.reshape([-1, 128, 128, 3])\n",
        "y_train_re = np.array(y_train)\n",
        "\n",
        "plot_label_clusters(vae, X_train_re, y_train_re)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbU3upiRIIKJ"
      },
      "source": [
        "plot_digits(X_val_reshape, y_val, encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdjgXrBdisPT"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sys\n",
        "\n",
        "import keras\n",
        "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, MaxPool2D, Flatten, BatchNormalization\n",
        "from keras.layers import Conv1D, MaxPool1D, CuDNNLSTM, Reshape, Conv1DTranspose\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Add, Concatenate\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.optimizers import SGD, Adam, RMSprop, Adadelta\n",
        "import keras.backend as K\n",
        "from keras.objectives import mean_squared_error\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer, RobustScaler, StandardScaler\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh0Qd0af_wPL"
      },
      "source": [
        "Variational Autoencodder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ao2H452_zb2"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import keras\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.losses import *\n",
        "from keras.callbacks import *\n",
        "from keras.optimizers import *\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAzHEmK2BAcy"
      },
      "source": [
        "# helper functions\n",
        "\n",
        "# reparameterization trick\n",
        "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
        "# z = z_mean + sqrt(var)*eps\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = tf.keras.backend.shape(z_mean)[0]\n",
        "    dim = tf.keras.backend.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean=0 and std=1.0\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "def plot_results(models, data, batch_size=128, model_name=\"vae_mnist\"):\n",
        "    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
        "    # Arguments:\n",
        "        models (tuple): encoder and decoder models\n",
        "        data (tuple): test data and label\n",
        "        batch_size (int): prediction batch size\n",
        "        model_name (string): which model is using this function\n",
        "    \"\"\"\n",
        "\n",
        "    encoder, decoder = models\n",
        "    x_test, y_test = data\n",
        "    os.makedirs(model_name, exist_ok=True)\n",
        "\n",
        "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, _, _ = encoder.predict(x_test,\n",
        "                                   batch_size=batch_size)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
        "    # display a 30x30 2D manifold of digits\n",
        "    n = 30\n",
        "    digit_size = 28\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    grid_x = np.linspace(-4, 4, n)\n",
        "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[i * digit_size: (i + 1) * digit_size,\n",
        "                   j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = n * digit_size + start_range + 1\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap='Greys_r')\n",
        "    plt.savefig(filename)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDku4gI4ELTU"
      },
      "source": [
        "# Reshape parameter to apply zmean\n",
        "\n",
        "X_train_va = np.reshape(X_train_img, [-1, 49152])\n",
        "X_val_va = np.reshape(X_val, [-1, 49152])\n",
        "X_train_va = X_train_va.astype('float32') / 255\n",
        "X_val_va = X_val_va.astype('float32') / 255\n",
        "\n",
        "# y_pred = stress_model.predict(X_val_reshape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdIa1yhNGmyc"
      },
      "source": [
        "original_dim = 49152\n",
        "# network parameters\n",
        "input_shape = (49152, )\n",
        "intermediate_dim = 512\n",
        "batch_size = 128\n",
        "latent_dim = 5\n",
        "epochs = 50\n",
        "\n",
        "# VAE model = encoder + decoder\n",
        "# build encoder model\n",
        "inputs = tf.keras.layers.Input(shape=input_shape, name='encoder_input')\n",
        "x = tf.keras.layers.Dense(intermediate_dim, activation='relu')(inputs)\n",
        "z_mean = tf.keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = tf.keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "# use reparameterization trick to push the sampling out as input\n",
        "z = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = tf.keras.models.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "# encoder.summary()\n",
        "# tf.keras.utils.plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
        "\n",
        "# build decoder model\n",
        "latent_inputs = tf.keras.layers.Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = tf.keras.layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
        "outputs = tf.keras.layers.Dense(original_dim, activation='softmax')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = tf.keras.models.Model(latent_inputs, outputs, name='decoder')\n",
        "# decoder.summary()\n",
        "# tf.keras.utils.plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
        "\n",
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = tf.keras.models.Model(inputs, outputs, name='vae_mlp')\n",
        "\n",
        "models = (encoder, decoder)\n",
        "# data = (x_test, y_test)\n",
        "# reconstruction_loss = tf.keras.losses.mse(inputs, outputs)\n",
        "reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, outputs)\n",
        "\n",
        "reconstruction_loss *= original_dim\n",
        "\n",
        "kl_loss = 1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var)\n",
        "kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\n",
        "# vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam', loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TjRo5tJHzfh"
      },
      "source": [
        "vae.fit(X_train_va, epochs=epochs, batch_size=batch_size, validation_data=(X_val_va, None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pop8qzPRZNxE"
      },
      "source": [
        "1D tests using wesad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u-yhnbuZQcD"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler \n",
        "from pandas import read_csv\n",
        "from numpy import set_printoptions\n",
        "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA, SparsePCA\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model, Sequential\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import preprocessing \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "np.random.seed(203)\n",
        "import scipy.sparse as sparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCn8mvub1xDj"
      },
      "source": [
        "filename = '/content/combined-swell-classification-hrv-test-dataset.csv'\n",
        "\n",
        "data = read_csv(filename)\n",
        "\n",
        "\n",
        "\n",
        "not_stressed_sw = data[data['Condition Label'] == 0].sample(6000)\n",
        "\n",
        "not_stressed_sw_L = data[data['Condition Label'] == 0].sample(500)\n",
        "\n",
        "\n",
        "\n",
        "stressed_sw =  data[data['Condition Label'] == 1]\n",
        "stressed2_sw = data[data['Condition Label'] == 2]\n",
        "stressed_sw = stressed_sw.append(stressed2_sw).sample(6000)\n",
        "\n",
        "stressed_sw_L = stressed_sw.append(stressed2_sw).sample(500)\n",
        "\n",
        "\n",
        "# stressed_SSSQ\n",
        "# y1 = df[:, 65]\n",
        "# y2 = df[:, 66]\n",
        "# y1_label = []\n",
        "# for i in y1:\n",
        "#   if i == 1:\n",
        "#     y1_label.append(1)\n",
        "#   elif i == 2:\n",
        "#     y1_label.append(1)\n",
        "#   else:\n",
        "#     y1_label.append(0)\n",
        "\n",
        "\n",
        "\n",
        "# for i in y2:\n",
        "#   if i == 1:\n",
        "#     y2_label.append(1)\n",
        "#   elif i == 2:\n",
        "#     y2_label.append(1)\n",
        "#   else:\n",
        "#     y2_label.append(0)\n",
        "\n",
        "\n",
        "df = not_stressed_sw.append(stressed_sw).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "df_sw_L = not_stressed_sw_L.append(stressed_sw_L).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "X = df.drop(['subject_id','condition','NasaTLX class','NasaTLX Label','Condition Label'], axis = 1).values\n",
        "\n",
        "X_sw_L = df_sw_L.drop(['subject_id','condition','NasaTLX class','NasaTLX Label','Condition Label'], axis = 1).values\n",
        "print(X)\n",
        "\n",
        "X_t = df.drop(['HR', 'MEAN_RR','RMSSD','condition','NasaTLX class','NasaTLX Label','Condition Label'], axis = 1).values\n",
        "X_flat = X_t.flatten()\n",
        "X_flat_pos = abs(X_flat)\n",
        "\n",
        "y1 = df[\"Condition Label\"].values\n",
        "y1 = np.where(y1>1, 1, y1)\n",
        "\n",
        "\n",
        "y1_sw_L = df_sw_L[\"Condition Label\"].values\n",
        "y1_sw_L = np.where(y1_sw_L>1, 1, y1_sw_L)\n",
        "\n",
        "\n",
        "\n",
        "print(y1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QoHDoYTablD"
      },
      "source": [
        "filename2 = '/content/wesad-classification-hrv-test-dataset.csv'\n",
        "\n",
        "data2 = read_csv(filename2)\n",
        "\n",
        "\n",
        "\n",
        "not_stressed2 = data2[data2['condition label'] == 0].sample(6000)\n",
        "not_stressed_L = data2[data2['condition label'] == 0].sample(500)\n",
        "\n",
        "\n",
        "\n",
        "stressed_2 =  data2[data2['condition label'] == 1]\n",
        "stressed_2_2 = data2[data2['condition label'] == 2]\n",
        "stressed_2 = stressed_2.append(stressed_2_2).sample(6000)\n",
        "stressed_2_L = stressed_2.append(stressed_2_2).sample(500)\n",
        "\n",
        "\n",
        "# stressed_SSSQ\n",
        "# y1 = df[:, 65]\n",
        "# y2 = df[:, 66]\n",
        "# y1_label = []\n",
        "# for i in y1:\n",
        "#   if i == 1:\n",
        "#     y1_label.append(1)\n",
        "#   elif i == 2:\n",
        "#     y1_label.append(1)\n",
        "#   else:\n",
        "#     y1_label.append(0)\n",
        "\n",
        "\n",
        "\n",
        "# for i in y2:\n",
        "#   if i == 1:\n",
        "#     y2_label.append(1)\n",
        "#   elif i == 2:\n",
        "#     y2_label.append(1)\n",
        "#   else:\n",
        "#     y2_label.append(0)\n",
        "\n",
        "\n",
        "df2 = not_stressed2.append(stressed_2).sample(frac=1).reset_index(drop=True)\n",
        "df_L = not_stressed_L.append(stressed_2_L).sample(frac = 1).reset_index(drop = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X1 = df2.drop(['subject id','condition','SSSQ class','SSSQ Label','condition label'], axis = 1).values\n",
        "print(X1)\n",
        "\n",
        "X_L = df_L.drop(['subject id','condition','SSSQ class','SSSQ Label','condition label'], axis = 1).values\n",
        "\n",
        "X1_flat = X1.flatten()\n",
        "X1_flat_pos = abs(X1_flat)\n",
        "\n",
        "\n",
        "y2 = df2[\"condition label\"].values\n",
        "y_L = df_L['condition label'].values\n",
        "y_L = np.where(y_L > 1, 1, y_L)\n",
        "\n",
        "\n",
        "y2 = np.where(y2>1, 1, y2)\n",
        "\n",
        "df3 = df2.drop(['subject id','condition','SSSQ class','SSSQ Label','condition label'], axis = 1)\n",
        "\n",
        "# df3['condition label'] = y2.tolist()\n",
        "print(df3.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc1_m0xkzgcB"
      },
      "source": [
        "Statistical Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6y3uO1gzjdJ"
      },
      "source": [
        "# T-test\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import ttest_rel\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.stats import mannwhitneyu\n",
        "from scipy.stats import wilcoxon\n",
        "from scipy.stats import friedmanchisquare\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.stats import chi2_contingency\n",
        "# from scipy.stats import kruska\n",
        "\n",
        "# stat, p = ttest_ind(X_flat, X1_flat)\n",
        "# print('Stat = %0.3f, p = %0.3f' % (stat, p))\n",
        "# if p > 0.05:\n",
        "#   print('the datasets are not different')\n",
        "# else:\n",
        "#   print('The dataset are different')\n",
        "\n",
        "stat_p, p_p = pearsonr(X_flat, X1_flat)\n",
        "print('Stat = %0.3f, p = %0.3f' % (stat_p, p_p))\n",
        "if p_p > 0.05:\n",
        "  print('The dataset are not different')\n",
        "else:\n",
        "  print('The dataset are different')\n",
        "\n",
        "\n",
        "\n",
        "stat_m, p_m = mannwhitneyu(X_flat, X1_flat)\n",
        "print('Stat_m = %0.3f, p_m = %0.3f' % (stat_m, p_m))\n",
        "if p_m > 0.05:\n",
        "  print('The dataset are not different')\n",
        "else:\n",
        "  print('The dataset are different')\n",
        "\n",
        "\n",
        "stat_w, p_w = wilcoxon(X_flat, X1_flat)\n",
        "print('Stat_w = %0.3f, p_w = %0.3f' % (stat_w, p_w))\n",
        "if p_w > 0.05:\n",
        "  print('The dataset are not different')\n",
        "else:\n",
        "  print('The dataset are different')\n",
        "\n",
        "stat_s, p_s = spearmanr(X_flat, X1_flat)\n",
        "print('stat_s = %0.3f, p_s = %0.3f' % (stat_s, p_s))\n",
        "if p_s > 0.05:\n",
        "  print('The dataset are not different')\n",
        "else:\n",
        "  print('The dataset are different')\n",
        "\n",
        "\n",
        "stat_c, p_c = chi2_contingency(X_flat_pos, X1_flat_pos)\n",
        "print('Stat_c = %0.3f, p_c = %0.3f' % (stat_c, p_c))\n",
        "if p_c > 0.05:\n",
        "  print('The dataset are not different')\n",
        "else:\n",
        "  print('The dataset are different ')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# stat_c, p_c = pearsonr(X_flat, X1_flat)\n",
        "# print('Stat = %0.3f, p = %0.3f' % (stat_c, p_c))\n",
        "# if p_c > 0.05:\n",
        "#   print('the datasets are not different')\n",
        "# else:\n",
        "#   print('The dataset are different')\n",
        "\n",
        "# stat_m, p_m = mannwhitneyu(X_flat, X1_flat)\n",
        "# print('Stat = %0.3f, p = %0.3f' % (stat_m, p_m))\n",
        "# if p_m > 0.05:\n",
        "#   print('the datasets are not different')\n",
        "# else:\n",
        "#   print('The dataset are different')\n",
        "\n",
        "# stat_w, p_w = wilcoxon(X_flat, X1_flat)\n",
        "# print('Stat = %0.3f, p = %0.3f' % (stat_w, p_w))\n",
        "# if p_w > 0.05:\n",
        "#   print('the datasets are not different')\n",
        "# else:\n",
        "#   print('The dataset are different')\n",
        "\n",
        "# stat_s, p_s = spearmanr(X_flat, X1_flat)\n",
        "# print('Stat = %0.3f, p = %0.3f' % (stat_s, p_s))\n",
        "# if p_s > 0.05:\n",
        "#   print('the datasets are not different')\n",
        "# else:\n",
        "#   print('The dataset are different')\n",
        "\n",
        "\n",
        "# stat_ch, p_ch = chi2_contingency(X_flat, X1_flat)\n",
        "# print('Stat = %0.3f, p = %0.3f' % (stat_ch, p_ch))\n",
        "# if p_ch > 0.05:\n",
        "#   print('the datasets are not different')\n",
        "# else:\n",
        "#   print('The dataset are different')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pubDFv9wIq_N"
      },
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from numpy import std\n",
        "from numpy import mean\n",
        "\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "cv = LeaveOneOut()\n",
        "\n",
        "train_x2, test_x2, train_y2, test_y2 = train_test_split(rep_x2, rep_y2, test_size=0.2, stratify=rep_y2,\n",
        "                     random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
        "RF = RandomForestClassifier(random_state=0) \n",
        "RF.fit(train_x2, train_y2)\n",
        "rf_pred = RF.predict(test_x2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# GB.fit(train_x2, train_y2)\n",
        "# gb_pred = GB.predict(test_x2)\n",
        "\n",
        "# rfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\n",
        "# rfecv = rfecv.fit(train_xd, train_yd)\n",
        "# cv = LeaveOneOut()\n",
        "# scores = cross_val_score(RF, rep_xL, rep_yL, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "\n",
        "# print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "\n",
        "# print('Optimal number of features :', rfecv.n_features_)\n",
        "# print('Best features :', df3.columns[rfecv.support_])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULUjaxNL3Zj2"
      },
      "source": [
        "def bias_variance_decomp(estimator, X_train, y_train, X_test, y_test,\n",
        "                         loss='0-1_loss', num_rounds=200, random_seed=None,\n",
        "                         **fit_params):\n",
        "    \"\"\"\n",
        "    estimator : object\n",
        "        A classifier or regressor object or class implementing both a\n",
        "        `fit` and `predict` method similar to the scikit-learn API.\n",
        "    X_train : array-like, shape=(num_examples, num_features)\n",
        "        A training dataset for drawing the bootstrap samples to carry\n",
        "        out the bias-variance decomposition.\n",
        "    y_train : array-like, shape=(num_examples)\n",
        "        Targets (class labels, continuous values in case of regression)\n",
        "        associated with the `X_train` examples.\n",
        "    X_test : array-like, shape=(num_examples, num_features)\n",
        "        The test dataset for computing the average loss, bias,\n",
        "        and variance.\n",
        "    y_test : array-like, shape=(num_examples)\n",
        "        Targets (class labels, continuous values in case of regression)\n",
        "        associated with the `X_test` examples.\n",
        "    loss : str (default='0-1_loss')\n",
        "        Loss function for performing the bias-variance decomposition.\n",
        "        Currently allowed values are '0-1_loss' and 'mse'.\n",
        "    num_rounds : int (default=200)\n",
        "        Number of bootstrap rounds (sampling from the training set)\n",
        "        for performing the bias-variance decomposition. Each bootstrap\n",
        "        sample has the same size as the original training set.\n",
        "    random_seed : int (default=None)\n",
        "        Random seed for the bootstrap sampling used for the\n",
        "        bias-variance decomposition.\n",
        "    fit_params : additional parameters\n",
        "        Additional parameters to be passed to the .fit() function of the\n",
        "        estimator when it is fit to the bootstrap samples.\n",
        "    Returns\n",
        "    ----------\n",
        "    avg_expected_loss, avg_bias, avg_var : returns the average expected\n",
        "        average bias, and average bias (all floats), where the average\n",
        "        is computed over the data points in the test set.\n",
        "    Examples\n",
        "    -----------\n",
        "    For usage examples, please see\n",
        "    http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/\n",
        "    \"\"\"\n",
        "    supported = ['0-1_loss', 'mse']\n",
        "    if loss not in supported:\n",
        "        raise NotImplementedError('loss must be one of the following: %s' %\n",
        "                                  supported)\n",
        "\n",
        "    for ary in (X_train, y_train, X_test, y_test):\n",
        "        if hasattr(ary, 'loc'):\n",
        "            raise ValueError('The bias_variance_decomp does not '\n",
        "                             'support pandas DataFrames yet. '\n",
        "                             'Please check the inputs to '\n",
        "                             'X_train, y_train, X_test, y_test. '\n",
        "                             'If e.g., X_train is a pandas '\n",
        "                             'DataFrame, try passing it as NumPy array via '\n",
        "                             'X_train=X_train.values.')\n",
        "\n",
        "    rng = np.random.RandomState(random_seed)\n",
        "\n",
        "    if loss == '0-1_loss':\n",
        "        dtype = np.int64\n",
        "    elif loss == 'mse':\n",
        "        dtype = np.float64\n",
        "\n",
        "    all_pred = np.zeros((num_rounds, y_test.shape[0]), dtype=dtype)\n",
        "\n",
        "    for i in range(num_rounds):\n",
        "        X_boot, y_boot = _draw_bootstrap_sample(rng, X_train, y_train)\n",
        "\n",
        "        # Keras support\n",
        "        if estimator.__class__.__name__ in ['Sequential', 'Functional']:\n",
        "\n",
        "            # reset model\n",
        "            for ix, layer in enumerate(estimator.layers):\n",
        "                if hasattr(estimator.layers[ix], 'kernel_initializer') and \\\n",
        "                        hasattr(estimator.layers[ix], 'bias_initializer'):\n",
        "                    weight_initializer = \\\n",
        "                        estimator.layers[ix].kernel_initializer\n",
        "                    bias_initializer = estimator.layers[ix].bias_initializer\n",
        "\n",
        "                    old_weights, old_biases = \\\n",
        "                        estimator.layers[ix].get_weights()\n",
        "\n",
        "                    estimator.layers[ix].set_weights([\n",
        "                        weight_initializer(shape=old_weights.shape),\n",
        "                        bias_initializer(shape=len(old_biases))])\n",
        "\n",
        "            estimator.fit(X_boot, y_boot, **fit_params)\n",
        "            pred = estimator.predict(X_test).reshape(1, -1)\n",
        "        else:\n",
        "            pred = estimator.fit(\n",
        "                X_boot, y_boot, **fit_params).predict(X_test)\n",
        "        all_pred[i] = pred\n",
        "\n",
        "    if loss == '0-1_loss':\n",
        "        main_predictions = np.apply_along_axis(lambda x:\n",
        "                                               np.argmax(np.bincount(x)),\n",
        "                                               axis=0,\n",
        "                                               arr=all_pred)\n",
        "\n",
        "        avg_expected_loss = np.apply_along_axis(lambda x:\n",
        "                                                (x != y_test).mean(),\n",
        "                                                axis=1,\n",
        "                                                arr=all_pred).mean()\n",
        "\n",
        "        avg_bias = np.sum(main_predictions != y_test) / y_test.size\n",
        "\n",
        "        var = np.zeros(pred.shape)\n",
        "\n",
        "        for pred in all_pred:\n",
        "            var += (pred != main_predictions).astype(np.int)\n",
        "        var /= num_rounds\n",
        "\n",
        "        avg_var = var.sum()/y_test.shape[0]\n",
        "\n",
        "    else:\n",
        "        avg_expected_loss = np.apply_along_axis(\n",
        "            lambda x:\n",
        "            ((x - y_test)**2).mean(),\n",
        "            axis=1,\n",
        "            arr=all_pred).mean()\n",
        "\n",
        "        main_predictions = np.mean(all_pred, axis=0)\n",
        "\n",
        "        avg_bias = np.sum((main_predictions - y_test)**2) / y_test.size\n",
        "        avg_var = np.sum((main_predictions - all_pred)**2) / all_pred.size\n",
        "\n",
        "    return avg_expected_loss, avg_bias, avg_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6aPGlkW4heQ"
      },
      "source": [
        "from mlxtend.evaluate import bias_variance_decomp\n",
        "\n",
        "mse, bias, var = bias_variance_decomp(RF, train_x2, train_y2, test_x2, test_y2, loss='mse', num_rounds=50, random_seed=None)\n",
        "# summarize results\n",
        "print('MSE: %.3f' % mse)\n",
        "print('Bias: %.3f' % bias)\n",
        "print('Variance: %.3f' % var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvTyZZUMj40a"
      },
      "source": [
        "from mlxtend.evaluate import bias_variance_decomp\n",
        "\n",
        "# Bias and Variances\n",
        "variance = np.var(rf_pred)\n",
        "print('Variance = %0.3f' % (variance))\n",
        "\n",
        "\n",
        "\n",
        "variance_gb = np.var(gb_pred)\n",
        "print(variance_gb)\n",
        "\n",
        "SSE_gb  = np.mean((np.mean(gb_pred) - rep_yL)**2)\n",
        "gb_bias = SSE_gb - variance_gb\n",
        "\n",
        "\n",
        "SSE = np.mean((np.mean(rf_pred) - rep_yL)**2)\n",
        "rf_bias = SSE - variance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Rf Bias: %0.3f' % (rf_bias))\n",
        "\n",
        "print('GB bias: %0.3f' % (gb_bias))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jex4rYhDkKKf"
      },
      "source": [
        "%pip install mlxtend --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjgL8a3iIclj"
      },
      "source": [
        "# Statistical test\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import std\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import log\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_xd, test_xd, train_yd, test_yd = train_test_split(X1, y2, test_size=0.2, stratify=y2,\n",
        "                     random_state=0)\n",
        "\n",
        "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
        "RF = RandomForestClassifier() \n",
        "RF.fit(train_xd, train_yd)\n",
        "num_params = 63\n",
        "yhat = RF.predict(test_xd)\n",
        "mse = mean_squared_error(test_yd, yhat)\n",
        "print('MSE: %.3f' % mse)\n",
        "\n",
        "\n",
        "# GB\n",
        "GB = GradientBoostingClassifier(n_estimators=100, \n",
        "                                learning_rate=1.0, \n",
        "                                max_depth=1, \n",
        "                                random_state=0)\n",
        "\n",
        "GB = GB.fit(train_xd, train_yd)\n",
        "pred_y = GB.predict(test_xd)\n",
        "mse_gb = mean_squared_error(test_yd, pred_y)\n",
        "print(mse_gb)\n",
        "\n",
        "\n",
        "\n",
        "# lower AIC and BIC means less information loss and overfitting\n",
        "def calculate_aic(n, mse, num_params):\n",
        "\taic = n * log(mse) + 2 * num_params\n",
        "\treturn aic\n",
        "\n",
        "\n",
        "aic = calculate_aic(len(train_yd), mse, num_params)\n",
        "\n",
        "aic_gb = calculate_aic(len(train_yd), mse_gb, num_params)\n",
        "\n",
        "print('AIC: %.3f' % aic)\n",
        "print('AIC for GB: %.3f' % aic_gb)\n",
        "\n",
        "def calculate_bic(n, mse, num_params):\n",
        "  bic = n * log(mse) + num_params * log(n)\n",
        "  return bic\n",
        "\n",
        "bic = calculate_bic(len(train_yd), mse, num_params)\n",
        "print('BIC: %.3f' % bic)\n",
        "\n",
        "bic_gb = calculate_bic(len(train_yd), mse_gb, num_params)\n",
        "print('BIC of GB: %.3f' % bic_gb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9LOMZlYVXZW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "plt.xlabel(\"Number of features selected\")\n",
        "plt.ylabel(\"Cross validation score of number of selected features\")\n",
        "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l6qGEIpX-BK"
      },
      "source": [
        "clf_rf_5 = RandomForestClassifier()      \n",
        "clr_rf_5 = clf_rf_5.fit(train_xd,train_yd)\n",
        "importances = clr_rf_5.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in clf_rf_5.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(train_xd.shape[1]):\n",
        "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
        "\n",
        "# Plot the feature importances of the forest\n",
        "\n",
        "plt.figure(1, figsize=(14, 13))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(train_xd.shape[1]), importances[indices],\n",
        "       color=\"g\", yerr=std[indices], align=\"center\")\n",
        "plt.xticks(range(train_xd.shape[1]), df3.columns[indices],rotation=90)\n",
        "plt.xlim([-1, train_xd.shape[1]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STX5_6Qn3rXk"
      },
      "source": [
        "# Train Data\n",
        "\n",
        "classes={0:\"Not Stressed\", 1:\"Stressed\"}\n",
        "plt.figure(figsize=(10,2))\n",
        "for i in range(0,2):\n",
        "    plt.subplot(1,2,i + 1)\n",
        "    all_samples_indexes = np.where(y1 == i)[0]\n",
        "    rand_samples_indexes = np.random.randint(0, len(all_samples_indexes), 3)\n",
        "    rand_samples = X[rand_samples_indexes]\n",
        "    plt.plot(rand_samples.transpose())\n",
        "    plt.title(\"Samples of class \" + classes[i], loc=\"left\", fontdict={'fontsize':8})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXcC69oG4edV"
      },
      "source": [
        "# Test Data\n",
        "# Train Data\n",
        "\n",
        "classes={0:\"Not Stressed\", 1:\"Stressed\"}\n",
        "plt.figure(figsize=(10,2))\n",
        "for i in range(0,2):\n",
        "    plt.subplot(1,2,i + 1)\n",
        "    all_samples_indexes = np.where(y2 == i)[0]\n",
        "    rand_samples_indexes = np.random.randint(0, len(all_samples_indexes), 3)\n",
        "    rand_samples = X1[rand_samples_indexes]\n",
        "    plt.plot(rand_samples.transpose())\n",
        "    plt.title(\"Samples of class \" + classes[i], loc=\"right\", fontdict={'fontsize':8})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IemJXZZJ4z3a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN6rkrjk3imb"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "corr = df2.corr()\n",
        "\n",
        "plt.figure(figsize= (100, 100))\n",
        "sns.heatmap(corr, annot = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn3b2QKBCzOK"
      },
      "source": [
        "# X_sup = df.loc[:,['MEAN_RR','MEDIAN_RR','HR','MEAN_RR_LOG','MEAN_RR_SQRT','HR_SQRT']]\n",
        "# print(X_sup)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMzgnkve5oCY"
      },
      "source": [
        "# X_sup2 = df2.loc[:,['MEAN_RR','MEDIAN_RR','HR','MEAN_RR_LOG','MEAN_RR_SQRT','HR_SQRT']].values\n",
        "\n",
        "# print(X_sup2)\n",
        "\n",
        "\n",
        "# X_sup2 = df2.iloc[:,[0,1,6,14,26,32,33,52,54,56]]\n",
        "X_sup2 = df2.iloc[:,[0,1,6,14,26,32]]\n",
        "print(X_sup2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOdeX6TjOsUp"
      },
      "source": [
        "def tsne_plot(x1, y1, name=\"graph.png\"):\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    X_t = tsne.fit_transform(x1)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.scatter(X_t[np.where(y1 == 0), 0], X_t[np.where(y1 == 0), 1], marker='o', color='g', linewidth='1', alpha=0.8, label='Not Stressed')\n",
        "    plt.scatter(X_t[np.where(y1 == 1), 0], X_t[np.where(y1 == 1), 1], marker='o', color='r', linewidth='1', alpha=0.8, label='Stressed')\n",
        "\n",
        "    plt.legend(loc='best');\n",
        "    plt.savefig(name);\n",
        "    plt.show();\n",
        "    \n",
        "tsne_plot(X, y1, \"original.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdU6oSHCs3LU"
      },
      "source": [
        "def tsne_plot(x1, y1, name=\"graph.png\"):\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    X_t = tsne.fit_transform(x1)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.scatter(X_t[np.where(y1 == 0), 0], X_t[np.where(y1 == 0), 1], marker='o', color='g', linewidth='1', alpha=0.8, label='Not Stressed')\n",
        "    plt.scatter(X_t[np.where(y1 == 1), 0], X_t[np.where(y1 == 1), 1], marker='o', color='r', linewidth='1', alpha=0.8, label='Stressed')\n",
        "\n",
        "    plt.legend(loc='best');\n",
        "    plt.savefig(name);\n",
        "    plt.show();\n",
        "    \n",
        "tsne_plot(X1, y2, \"original.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbAgzhpIDmzV"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzGUVfjThY5Q"
      },
      "source": [
        "# PCA\n",
        "pca = KernelPCA(n_components=4)\n",
        "fit = pca.fit(X_sup)\n",
        "X_sup_new = pca.transform(X_sup)\n",
        "print(X_sup_new.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZxaIgliPyln"
      },
      "source": [
        "pca2 = KernelPCA(n_components=4)\n",
        "fit = pca2.fit(X_sup2)\n",
        "X_sup2_new = pca2.transform(X_sup2)\n",
        "print(X_sup2_new.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU_Coo9CKl87"
      },
      "source": [
        "sparsepca = make_pipeline(StandardScaler(),\n",
        "                          SparsePCA(n_components=9,\n",
        "                                    alpha=0.0001,\n",
        "                                    random_state=0,\n",
        "                                    n_jobs=-1))\n",
        "fit = sparsepca.fit(X_sup2)\n",
        "X_sup2_new = sparsepca.transform(X_sup2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ywuZf1VSlAC"
      },
      "source": [
        "def plot_hist(class_number,size,min_, bins):\n",
        "    img=df3.loc[df3['condition label']==class_number].values\n",
        "    img=img[:,min_:size]\n",
        "    img_flatten=img.flatten()\n",
        "    print(img.shape)\n",
        "    final1=np.arange(min_,size)\n",
        "    print(final1.shape)\n",
        "    for i in range (img.shape[0]-1):\n",
        "        tempo1=np.arange(min_,size)\n",
        "        final1=np.concatenate((final1, tempo1), axis=None)\n",
        "    print(len(final1))\n",
        "    print(len(img_flatten))\n",
        "    plt.hist2d(final1, img_flatten, bins=(bins),cmap=plt.cm.jet)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu9DhXrtS9-c"
      },
      "source": [
        "plot_hist(0, 10,0, 10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWqo4EnFeW8J"
      },
      "source": [
        "plot_hist(1, 10, 0, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYkck8gdBFS_"
      },
      "source": [
        "Scatter Plot using various dimensional reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0VyvQLgBXHW"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "from sklearn.decomposition import (PCA, IncrementalPCA,\n",
        "                                   KernelPCA, TruncatedSVD,\n",
        "                                   FastICA, MiniBatchDictionaryLearning,\n",
        "                                   SparsePCA)\n",
        "\n",
        "from sklearn.manifold import (Isomap,\n",
        "                              LocallyLinearEmbedding)\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "from sklearn.random_projection import (GaussianRandomProjection,\n",
        "                                       SparseRandomProjection)\n",
        "\n",
        "from sklearn.neighbors import (KNeighborsClassifier,\n",
        "                               NeighborhoodComponentsAnalysis)\n",
        "                               \n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBJU10nQBqYt"
      },
      "source": [
        "# Train, Test split for dimensional reduction\n",
        "\n",
        "train_xd, test_xd, train_yd, test_yd = train_test_split(X_sup2, y2, test_size=0.2, stratify=y2,\n",
        "                     random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUPYcymBEIle"
      },
      "source": [
        "random_state = 0\n",
        "\n",
        "pca = make_pipeline(StandardScaler(),\n",
        "                    PCA(n_components=4,\n",
        "                        random_state=random_state))\n",
        "\n",
        "inc_pca = make_pipeline(StandardScaler(),\n",
        "                        IncrementalPCA(n_components=4))\n",
        "\n",
        "# kernel : linear | poly | rbf | sigmoid | cosine | precomputed\n",
        "kpca = make_pipeline(StandardScaler(),\n",
        "                     KernelPCA(kernel=\"cosine\",\n",
        "                               n_components=4,\n",
        "                               gamma=None,\n",
        "                               fit_inverse_transform=True,\n",
        "                               random_state=random_state,\n",
        "                               n_jobs=1))\n",
        "\n",
        "sparsepca = make_pipeline(StandardScaler(),\n",
        "                          SparsePCA(n_components=4,\n",
        "                                    alpha=0.0001,\n",
        "                                    random_state=random_state,\n",
        "                                    n_jobs=-1))\n",
        "\n",
        "\n",
        "SVD = make_pipeline(StandardScaler(),\n",
        "                    TruncatedSVD(n_components=4,\n",
        "                                 algorithm='randomized',\n",
        "                                 random_state=random_state,\n",
        "                                 n_iter=5))\n",
        "\n",
        "\n",
        "GRP = make_pipeline(StandardScaler(),\n",
        "                    GaussianRandomProjection(n_components=4,\n",
        "                                             eps = 0.5,\n",
        "                                             random_state=random_state))\n",
        "\n",
        "# lda = make_pipeline(StandardScaler(),\n",
        "#                     LinearDiscriminantAnalysis(n_components=4))\n",
        "\n",
        "nca = make_pipeline(StandardScaler(),\n",
        "                    NeighborhoodComponentsAnalysis(n_components=4,\n",
        "                                                   random_state=random_state))\n",
        "\n",
        "\n",
        "SRP = make_pipeline(StandardScaler(),\n",
        "                    SparseRandomProjection(n_components=4,\n",
        "                                           density = 'auto',\n",
        "                                           eps = 0.5,\n",
        "                                           random_state=random_state,\n",
        "                                           dense_output = False))\n",
        "\n",
        "\n",
        "isomap = make_pipeline(StandardScaler(),\n",
        "                       Isomap(n_components=4,\n",
        "                              n_jobs = 4,\n",
        "                              n_neighbors = 5))\n",
        "\n",
        "\n",
        "miniBatchDictLearning = make_pipeline(StandardScaler(),\n",
        "                                      MiniBatchDictionaryLearning(n_components=4,\n",
        "                                                                  batch_size = 200,\n",
        "                                                                  alpha = 1,\n",
        "                                                                  n_iter = 25,\n",
        "                                                                  random_state=random_state))\n",
        "\n",
        "\n",
        "# FastICA = make_pipeline(StandardScaler(),\n",
        "#                         FastICA(n_components=3,\n",
        "#                                 algorithm = 'parallel',\n",
        "#                                 whiten = True,\n",
        "#                                 max_iter = 100,\n",
        "#                                 random_state=random_state))\n",
        "\n",
        "\n",
        "lle = make_pipeline(StandardScaler(),\n",
        "                    LocallyLinearEmbedding(n_components=4,\n",
        "                                           n_neighbors = 10,\n",
        "                                           method = 'modified',\n",
        "                                           n_jobs = 4,\n",
        "                                           random_state=random_state))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDi5Oa5SBJw9"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# train_x, val_x, train_y, val_y = train_test_split(_x, rep_y, test_size=0.25)\n",
        "# GB = GradientBoostingClassifier(n_estimators=100, \n",
        "#                                 learning_rate=1.0, \n",
        "#                                 max_depth=1, \n",
        "#                                 random_state=0)\n",
        "\n",
        "\n",
        "# Make a list of the methods to be compared\n",
        "dim_reduction_methods = {'PCA': pca, \n",
        "                        #  'LDA': lda, \n",
        "                         'NCA': nca, \n",
        "                         'INC PCA': inc_pca, \n",
        "                         'KPCA':kpca, \n",
        "                         'Sparced PCA': sparsepca, \n",
        "                         'SVD': SVD, \n",
        "                         'GRP' : GRP, \n",
        "                         'SRP': SRP, \n",
        "                         'IsoMap': isomap, \n",
        "                         'MBD': miniBatchDictLearning, \n",
        "                        #  'ICA': FastICA, \n",
        "                         'LLE': lle}\n",
        "\n",
        "\n",
        "plt.figure(figsize=(24, 36))\n",
        "\n",
        "for j,(name, model) in enumerate(dim_reduction_methods.items()):\n",
        "    plt.subplot(5, 3, j + 1, aspect='auto')\n",
        "\n",
        "    # Fit the method's model\n",
        "    model.fit(train_xd, train_yd)\n",
        "\n",
        "    # Fit a nearest neighbor classifier on the embedded training set\n",
        "    clf_rf_4.fit(model.transform(train_xd), train_yd)\n",
        "\n",
        "    # Compute the nearest neighbor accuracy on the embedded test set\n",
        "    # acc_GB = GB.score(model.transform(test_xd), test_yd)\n",
        "    acc_RF = clf_rf_4.score(model.transform(test_xd), test_yd)\n",
        "\n",
        "    # Fit the methons using the fitted model\n",
        "    X_embedded = model.transform(X_sup2)\n",
        "    print(X_embedded.shape)\n",
        "    \n",
        "    # Creating a dataframe to easily plot the sample label\n",
        "    df = pd.DataFrame(np.concatenate((X_embedded, np.reshape(y2, (-1, 1))), axis=1))\n",
        "\n",
        "    # Plot the projected points and show the evaluation score\n",
        "    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y2, s=20, cmap='Set1')\n",
        "    plt.title(\"{}, RF Test accuracy = {:.2f}\".format(name, acc_RF))\n",
        "    plt.colorbar()\n",
        "    \n",
        "    # Label the data distributions\n",
        "    for i, number in enumerate(test_yd):\n",
        "        plt.annotate(number,\n",
        "                     df.loc[df[2]==number,[0,1]].mean(),\n",
        "                     horizontalalignment='center',\n",
        "                     verticalalignment='center',\n",
        "                     weight='bold',\n",
        "                     size='20')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INEoHhUIEw-7"
      },
      "source": [
        "## input layer \n",
        "input_layer = Input(shape=(X.shape[1],))\n",
        "\n",
        "## encoding part\n",
        "encoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-4))(input_layer)\n",
        "encoded = Dense(50, activation='relu')(encoded)\n",
        "# encoded = Dense(25, activation='relu')(encoded)\n",
        "\n",
        "\n",
        "## decoding part\n",
        "# encoded = Dense(25, activation='relu')(encoded)\n",
        "decoded = Dense(50, activation='tanh')(encoded)\n",
        "decoded = Dense(100, activation='tanh')(decoded)\n",
        "\n",
        "## output layer\n",
        "output_layer = Dense(X.shape[1], activation='relu')(decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSbCvvUaQO3o"
      },
      "source": [
        "## input layer \n",
        "input_layer2 = Input(shape=(X_sup2.shape[1],))\n",
        "\n",
        "## encoding part\n",
        "encoded2 = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-4))(input_layer2)\n",
        "encoded2 = Dense(50, activation='relu')(encoded2)\n",
        "# encoded2 = Dense(25, activation='relu')(encoded2)\n",
        "\n",
        "\n",
        "## decoding part\n",
        "# encoded2 = Dense(25, activation='relu')(encoded2)\n",
        "decoded2 = Dense(50, activation='tanh')(encoded2)\n",
        "decoded2 = Dense(100, activation='tanh')(decoded2)\n",
        "\n",
        "## output layer\n",
        "output_layer2 = Dense(X_sup2.shape[1], activation='relu')(decoded2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4oFz-jciLLc"
      },
      "source": [
        "# VGG\n",
        "## input layer \n",
        "input_layer2 = Input(shape=(X1.shape[1],))\n",
        "\n",
        "## encoding part\n",
        "encoded2 = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-4))(input_layer2)\n",
        "encoded2 = Dense(50, activation='relu')(encoded2)\n",
        "# encoded2 = Dense(25, activation='relu')(encoded2)\n",
        "\n",
        "\n",
        "## decoding part\n",
        "# encoded2 = Dense(25, activation='relu')(encoded2)\n",
        "decoded2 = Dense(50, activation='tanh')(encoded2)\n",
        "decoded2 = Dense(100, activation='tanh')(decoded2)\n",
        "\n",
        "## output layer\n",
        "output_layer2 = Dense(X1.shape[1], activation='relu')(decoded2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfgPfA-o7HW0"
      },
      "source": [
        "# LOOCV\n",
        "## input layer \n",
        "input_layer_L = Input(shape=(X_L.shape[1],))\n",
        "\n",
        "## encoding part\n",
        "encoded2 = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-4))(input_layer_L)\n",
        "encoded2 = Dense(50, activation='relu')(encoded2)\n",
        "# encoded2 = Dense(25, activation='relu')(encoded2)\n",
        "\n",
        "\n",
        "## decoding part\n",
        "# encoded2 = Dense(25, activation='relu')(encoded2)\n",
        "decoded2 = Dense(50, activation='tanh')(encoded2)\n",
        "decoded2 = Dense(100, activation='tanh')(decoded2)\n",
        "\n",
        "## output layer\n",
        "output_layer_L = Dense(X_L.shape[1], activation='relu')(decoded2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQHSySvKE2TW"
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_adabound import AdaBound\n",
        "from keras_radam import RAdam\n",
        "\n",
        "autoencoder = Model(input_layer, output_layer)\n",
        "# opt = SGD(lr = 0.01, momentum = 0.9)\n",
        "opt = AdaBound(lr=0.01, final_lr=0.1, gamma = 0.01, weight_decay= 0.)\n",
        "# opt = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=0.1)\n",
        "autoencoder.compile(optimizer=opt, loss=\"mse\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWudcFvU7htC"
      },
      "source": [
        "# LOOCV autoencoder\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_adabound import AdaBound\n",
        "from keras_radam import RAdam\n",
        "\n",
        "autoencoder_L = Model(input_layer_L, output_layer_L)\n",
        "# opt = SGD(lr = 0.01, momentum = 0.9)\n",
        "opt = AdaBound(lr=0.01, final_lr=0.1, gamma = 0.01, weight_decay= 0.)\n",
        "# opt = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=0.1)\n",
        "autoencoder_L.compile(optimizer=opt, loss=\"mse\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el05sZ3YiiIn"
      },
      "source": [
        "pip install keras-adabound"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm3LSNoTjeLL"
      },
      "source": [
        "pip install keras-rectified-adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdKRZYkqQgZP"
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_adabound import AdaBound\n",
        "from keras_radam import RAdam\n",
        "\n",
        "autoencoder2 = Model(input_layer2, output_layer2)\n",
        "# opt = SGD(lr = 0.01, momentum = 0.9)\n",
        "opt = AdaBound(lr=0.01, final_lr=0.1, gamma = 0.01, weight_decay= 0.)\n",
        "opt2 = RAdam(total_steps=10000, warmup_proportion=0.01, min_lr=0.001)\n",
        "opt3 = Adam(lr = 0.001)\n",
        "opt4 = SGD(lr = 0.001, momentum = 0.9)\n",
        "opt5 = 'adadelta'\n",
        "# opt = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=0.01)\n",
        "autoencoder2.compile(optimizer=opt, loss=\"mse\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a_AD4Dcj-Bc"
      },
      "source": [
        "import time\n",
        "\n",
        "# Make a list of the methods to be compared\n",
        "optimizers = {'RAdam': opt2, \n",
        "                         'Adam': opt3, \n",
        "                         'SGD':opt4, \n",
        "                         'Adadelta': opt5, \n",
        "                         'AdaBound': opt, \n",
        "                        #  'LDA\n",
        "                         }\n",
        "\n",
        "\n",
        "plt.figure(figsize=(24, 36))\n",
        "plt.title(\"Visualizing loss with various optimizers (StandardScaler)\")\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "for j,(name, opt) in enumerate(optimizers.items()):\n",
        "    # plt.subplot(5, 3, j + 1, aspect='auto')\n",
        "    \n",
        "\n",
        "    # Fit the method's model\n",
        "    autoencoder2.compile(optimizer = opt, loss = 'mse')\n",
        "    # x_scale2 = StandardScaler().fit_transform(X1)\n",
        "    # x_norm2, x_stress2 = x_scale2[y2 == 0], x_scale2[y2 == 1]\n",
        "\n",
        "    history = autoencoder2.fit(x_norm2[0:12000], x_norm2[0:12000], \n",
        "                batch_size = 256, epochs = 50, \n",
        "                shuffle = True, validation_split = 0.20);\n",
        "\n",
        "    \n",
        "    # plt.plot(history.history['loss'])\n",
        "\n",
        "    plt.plot(history.history['val_loss'], label = \"val_loss of \" + str(name))\n",
        "    plt.legend()\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "\n",
        "    time.sleep(3)\n",
        "plt.show()\n",
        "    \n",
        "    \n",
        "    \n",
        "#     model.fit(train_xd, train_yd)\n",
        "\n",
        "#     # Fit a nearest neighbor classifier on the embedded training set\n",
        "#     clf_rf_4.fit(model.transform(train_xd), train_yd)\n",
        "\n",
        "#     # Compute the nearest neighbor accuracy on the embedded test set\n",
        "#     # acc_GB = GB.score(model.transform(test_xd), test_yd)\n",
        "#     acc_RF = clf_rf_4.score(model.transform(test_xd), test_yd)\n",
        "\n",
        "#     # Fit the methons using the fitted model\n",
        "#     X_embedded = model.transform(X_sup2)\n",
        "#     print(X_embedded.shape)\n",
        "    \n",
        "#     # Creating a dataframe to easily plot the sample label\n",
        "#     df = pd.DataFrame(np.concatenate((X_embedded, np.reshape(y2, (-1, 1))), axis=1))\n",
        "\n",
        "#     # Plot the projected points and show the evaluation score\n",
        "#     plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y2, s=20, cmap='Set1')\n",
        "#     plt.title(\"{}, RF Test accuracy = {:.2f}\".format(name, acc_RF))\n",
        "#     plt.colorbar()\n",
        "    \n",
        "#     # Label the data distributions\n",
        "#     for i, number in enumerate(test_yd):\n",
        "#         plt.annotate(number,\n",
        "#                      df.loc[df[2]==number,[0,1]].mean(),\n",
        "#                      horizontalalignment='center',\n",
        "#                      verticalalignment='center',\n",
        "#                      weight='bold',\n",
        "#                      size='20')\n",
        "    \n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K26oTQsyFCB1"
      },
      "source": [
        "x_scale = MinMaxScaler().fit_transform(X)\n",
        "x_norm, x_stress = x_scale[y1 == 0], x_scale[y1 == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-RA1nZlQnE1"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "# x_scale2 = MinMaxScaler().fit_transform(X_sup2)\n",
        "x_scale2 = MinMaxScaler().fit_transform(X1)\n",
        "# x_scale2 = StandardScaler().fit_transform(X1)\n",
        "x_norm2, x_stress2 = x_scale2[y2 == 0], x_scale2[y2 == 1]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd312RN47rla"
      },
      "source": [
        "# LOOCV autoencoder\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "# x_scale2 = MinMaxScaler().fit_transform(X_sup2)\n",
        "x_scale_L = MinMaxScaler().fit_transform(X_L)\n",
        "# x_scale2 = StandardScaler().fit_transform(X1)\n",
        "x_norm_L, x_stress_L = x_scale_L[y_L == 0], x_scale_L[y_L == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m8dQYYCq1ER"
      },
      "source": [
        "autoencoder.fit(x_norm[0:12000], x_norm[0:12000], \n",
        "                batch_size = 256, epochs = 50, \n",
        "                shuffle = True, validation_split = 0.20);\n",
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c60jw4MRFkd"
      },
      "source": [
        "# autoencoder2.fit(x_norm2[0:6000], x_norm2[0:6000], \n",
        "#                 batch_size = 256, epochs = 50, \n",
        "#                 shuffle = True, validation_split = 0.20);\n",
        "# autoencoder2.summary()\n",
        "\n",
        "autoencoder2.fit(x_norm2[0:12000], x_norm2[0:12000], \n",
        "                batch_size = 256, epochs = 50, \n",
        "                shuffle = True, validation_split = 0.20);\n",
        "autoencoder2.summary()\n",
        "# plot_model(autoencoder2, to_file='autoencoder_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJe-fSaO74YP"
      },
      "source": [
        "# LOOCV autoencoder\n",
        "\n",
        "autoencoder_L.fit(x_norm_L[0:1000], x_norm_L[0:1000], \n",
        "                batch_size = 128, epochs = 50, \n",
        "                shuffle = True, validation_split = 0.20);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuGku0QP9RM7"
      },
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "plt.title(\"Visualizing loss with various Adabound\")\n",
        "# fig, ax = plt.subplots(figsize=(5,5))\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'], label = \"val_loss\") \n",
        "plt.legend()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "\n",
        "  \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seScpyJ4FjCj"
      },
      "source": [
        "hidden_representation = Sequential()\n",
        "hidden_representation.add(autoencoder.layers[0])\n",
        "hidden_representation.add(autoencoder.layers[1])\n",
        "hidden_representation.add(autoencoder.layers[2])\n",
        "hidden_representation.add(autoencoder.layers[3])\n",
        "# hidden_representation.add(autoencoder.layers[4])\n",
        "# hidden_representation.add(autoencoder.layers[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCcM3dOuSZyX"
      },
      "source": [
        "hidden_representation2 = Sequential()\n",
        "hidden_representation2.add(autoencoder2.layers[0])\n",
        "hidden_representation2.add(autoencoder2.layers[1])\n",
        "hidden_representation2.add(autoencoder2.layers[2])\n",
        "hidden_representation2.add(autoencoder2.layers[3])\n",
        "# hidden_representation2.add(autoencoder2.layers[4])\n",
        "# hidden_representation2.add(autoencoder2.layers[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LPgDxWR8bGS"
      },
      "source": [
        "# LOOCV autoencoder\n",
        "\n",
        "hidden_representation_L = Sequential()\n",
        "hidden_representation_L.add(autoencoder_L.layers[0])\n",
        "hidden_representation_L.add(autoencoder_L.layers[1])\n",
        "hidden_representation_L.add(autoencoder_L.layers[2])\n",
        "hidden_representation_L.add(autoencoder_L.layers[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91wE5M_SFsgB"
      },
      "source": [
        "norm_hid_rep = hidden_representation.predict(x_norm[:12000])\n",
        "stress_hid_rep = hidden_representation.predict(x_stress)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grBbbEsISg1I"
      },
      "source": [
        "# norm_hid_rep2 = hidden_representation2.predict(x_norm2[:6000])\n",
        "# stress_hid_rep2 = hidden_representation2.predict(x_stress2)\n",
        "\n",
        "norm_hid_rep2 = hidden_representation2.predict(x_norm2[:12000])\n",
        "stress_hid_rep2 = hidden_representation2.predict(x_stress2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3GwOkQJ8mdL"
      },
      "source": [
        "# LOOCV autoencoder\n",
        "# norm_hid_rep2 = hidden_representation2.predict(x_norm2[:6000])\n",
        "# stress_hid_rep2 = hidden_representation2.predict(x_stress2)\n",
        "\n",
        "norm_hid_rep_L = hidden_representation_L.predict(x_norm_L[:1000])\n",
        "stress_hid_rep_L = hidden_representation_L.predict(x_stress_L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0B069muF5ZL"
      },
      "source": [
        "rep_x = np.append(norm_hid_rep, stress_hid_rep, axis = 0)\n",
        "y_norm = np.zeros(norm_hid_rep.shape[0])\n",
        "y_stress = np.ones(stress_hid_rep.shape[0])\n",
        "rep_y = np.append(y_norm, y_stress)\n",
        "tsne_plot(rep_x, rep_y, \"latent_representation.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrqG0iKsSo2S"
      },
      "source": [
        "rep_x2 = np.append(norm_hid_rep2, stress_hid_rep2, axis = 0)\n",
        "y_norm2 = np.zeros(norm_hid_rep2.shape[0])\n",
        "y_stress2 = np.ones(stress_hid_rep2.shape[0])\n",
        "rep_y2 = np.append(y_norm2, y_stress2)\n",
        "tsne_plot(rep_x2, rep_y2, \"latent_representation.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlP7vJrg8zWF"
      },
      "source": [
        "# LOOOCV autoencoder data\n",
        "\n",
        "rep_xL = np.append(norm_hid_rep_L, stress_hid_rep_L, axis = 0)\n",
        "y_norm_L = np.zeros(norm_hid_rep_L.shape[0])\n",
        "y_stress_L = np.ones(stress_hid_rep_L.shape[0])\n",
        "rep_yL = np.append(y_norm_L, y_stress_L)\n",
        "tsne_plot(rep_xL, rep_yL, \"latent_representation.png\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e-VCOHoGXsl"
      },
      "source": [
        "# LR classification\n",
        "\n",
        "train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.25)\n",
        "clf = LogisticRegression(solver=\"lbfgs\").fit(train_x, train_y)\n",
        "pred_y = clf.predict(val_x)\n",
        "\n",
        "print (\"Accuracy Score: \", accuracy_score(val_y, pred_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NeX-focq5hy"
      },
      "source": [
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.linear_model import Ridge\n",
        "from yellowbrick.model_selection import LearningCurve\n",
        "\n",
        "train_x2, val_x2, train_y2, val_y2 = train_test_split(rep_x2, rep_y2, test_size=0.25)\n",
        "RF.fit(train_x2, train_y2)\n",
        "pred_yd = RF.predict(val_x2)\n",
        "\n",
        "print (\"Accuracy Score: \", accuracy_score(val_y2, pred_yd))\n",
        "\n",
        "\n",
        "precision = precision_score(val_y2, pred_yd)\n",
        "print('Precision: %f' % precision)\n",
        "\n",
        "\n",
        "recall = recall_score(val_y2, pred_yd)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(val_y2, pred_yd)\n",
        "print('F1 score: %f' % f1)\n",
        " \n",
        "# kappa\n",
        "kappa = cohen_kappa_score(val_y2, pred_yd)\n",
        "print('Cohens kappa: %f' % kappa)\n",
        "# ROC AUC\n",
        "auc = roc_auc_score(val_y2, pred_yd)\n",
        "print('ROC AUC: %f' % auc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO61ZD5mU30n"
      },
      "source": [
        "train_x2, val_x2, train_y2, val_y2 = train_test_split(rep_x2, rep_y2, test_size=0.25)\n",
        "clf2 = LogisticRegression(solver=\"lbfgs\").fit(train_x2, train_y2)\n",
        "pred_y2 = clf2.predict(val_x2)\n",
        "\n",
        "print (\"Accuracy Score: \", accuracy_score(val_y2, pred_y2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I8zsq0FMS0K"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.25)\n",
        "GB = GradientBoostingClassifier(n_estimators=100, \n",
        "                                learning_rate=1.0, \n",
        "                                max_depth=1, \n",
        "                                random_state=0)\n",
        "\n",
        "GB_clf = GB.fit(train_x, train_y)\n",
        "pred_y = GB_clf.predict(val_x)\n",
        "\n",
        "print (\"Accuracy Score: \", accuracy_score(val_y, pred_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yqVTdrZVhpx"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "train_x2, val_x2, train_y2, val_y2 = train_test_split(rep_x2, rep_y2, test_size=0.25)\n",
        "GB = GradientBoostingClassifier(n_estimators=100, \n",
        "                                learning_rate=1.0, \n",
        "                                max_depth=1, \n",
        "                                random_state=0)\n",
        "\n",
        "GB_clf2 = GB.fit(train_x2, train_y2)\n",
        "pred_y2 = GB_clf2.predict(val_x2)\n",
        "hat_y2 = GB_clf2.predict(val_x2)\n",
        "\n",
        "print (\"Accuracy Score: \", accuracy_score(val_y2, pred_y2))\n",
        "\n",
        "precision = precision_score(val_y2, hat_y2)\n",
        "print('Precision: %f' % precision)\n",
        "\n",
        "\n",
        "recall = recall_score(val_y2, hat_y2)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(val_y2, hat_y2)\n",
        "print('F1 score: %f' % f1)\n",
        " \n",
        "# kappa\n",
        "kappa = cohen_kappa_score(val_y2, hat_y2)\n",
        "print('Cohens kappa: %f' % kappa)\n",
        "# ROC AUC\n",
        "auc = roc_auc_score(val_y2, hat_y2)\n",
        "print('ROC AUC: %f' % auc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRCOK1JlHsj3"
      },
      "source": [
        "# NN classification\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=50, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_x, train_y, epochs=50, batch_size=10)\n",
        "_, accuracy = model.evaluate(val_x, val_y)\n",
        "\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txs-zWrAlp60"
      },
      "source": [
        "# opt2 = SGD(lr = 0.0001, momentum = 0.9)\n",
        "# opt2 = AdaBound(lr=0.0001, final_lr=0.001, gamma = 0.01, weight_decay= 0.)\n",
        "opt2 = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv4XNquGIcGK"
      },
      "source": [
        "# univariate cnn example\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Dense, Convolution1D, MaxPool1D, Flatten, Dropout\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def network(X_train,y_train,X_test,y_test):\n",
        "    \n",
        "\n",
        "    im_shape=(X_train.shape[1],1)\n",
        "    inputs_cnn=Input(shape=(im_shape), name='inputs_cnn')\n",
        "    conv1_1=Convolution1D(64, (6), activation='relu', input_shape=im_shape)(inputs_cnn)\n",
        "    conv1_1=BatchNormalization()(conv1_1)\n",
        "    pool1=MaxPool1D(pool_size=(3), strides=(2), padding=\"same\")(conv1_1)\n",
        "    conv2_1=Convolution1D(64, (6), activation='relu', input_shape=im_shape)(pool1)\n",
        "    conv2_1=BatchNormalization()(conv2_1)\n",
        "    pool2=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv2_1)\n",
        "    conv3_1=Convolution1D(64, (3), activation='relu', input_shape=im_shape)(pool2)\n",
        "    conv3_1=BatchNormalization()(conv3_1)\n",
        "    pool3=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv3_1)\n",
        "    flatten=Flatten()(pool3)\n",
        "    dense_end1 = Dense(64, activation='relu')(flatten)\n",
        "    dense_end2 = Dense(32, activation='relu')(dense_end1)\n",
        "    main_output = Dense(1, activation='sigmoid', name='main_output')(dense_end2)\n",
        "    \n",
        "    \n",
        "    model = Model(inputs= inputs_cnn, outputs=main_output)\n",
        "    model.compile(optimizer='adam', loss='mse',metrics = ['accuracy'])\n",
        "    \n",
        "    \n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
        "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "\n",
        "    history=model.fit(X_train, y_train,epochs=40,callbacks=callbacks, batch_size=32,validation_data=(X_test,y_test))\n",
        "    model.load_weights('best_model.h5')\n",
        "    return(model,history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItJeD2beOJlx"
      },
      "source": [
        "# Using RFE features only\n",
        "# univariate cnn example\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Dense, Convolution1D, MaxPool1D, Flatten, Dropout\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def network(X_train,y_train,X_test,y_test):\n",
        "    \n",
        "\n",
        "    im_shape=(X_train.shape[1],1)\n",
        "    inputs_cnn=Input(shape=(im_shape), name='inputs_cnn')\n",
        "    conv1_1=Convolution1D(64, (1), activation='relu', input_shape=im_shape)(inputs_cnn)\n",
        "    conv1_1=BatchNormalization()(conv1_1)\n",
        "    pool1=MaxPool1D(pool_size=(3), strides=(2), padding=\"same\")(conv1_1)\n",
        "    conv2_1=Convolution1D(64, (1), activation='relu', input_shape=im_shape)(pool1)\n",
        "    conv2_1=BatchNormalization()(conv2_1)\n",
        "    pool2=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv2_1)\n",
        "    conv3_1=Convolution1D(64, (1), activation='relu', input_shape=im_shape)(pool2)\n",
        "    conv3_1=BatchNormalization()(conv3_1)\n",
        "    pool3=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv3_1)\n",
        "    flatten=Flatten()(pool3)\n",
        "    dense_end1 = Dense(64, activation='relu')(flatten)\n",
        "    dense_end2 = Dense(32, activation='relu')(dense_end1)\n",
        "    main_output = Dense(1, activation='sigmoid', name='main_output')(dense_end2)\n",
        "    \n",
        "    \n",
        "    model = Model(inputs= inputs_cnn, outputs=main_output)\n",
        "    model.compile(optimizer='adam', loss='mse',metrics = ['accuracy'])\n",
        "    \n",
        "    \n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
        "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "\n",
        "    history=model.fit(X_train, y_train,epochs=40,callbacks=callbacks, batch_size=32,validation_data=(X_test,y_test))\n",
        "    model.load_weights('best_model.h5')\n",
        "    return(model,history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iei68qXTO11W"
      },
      "source": [
        "1D VGG 16 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXiRrmikHl25"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.initializers import RandomUniform\n",
        "\n",
        "# train_x, val_x, train_y, val_y = train_test_split(rep_x2, rep_y2, test_size=0.25)\n",
        "\n",
        "def make_vgg161D_model(X_train, y_train, X_test, y_test):\n",
        "  I = (X_train.shape[1], 1)\n",
        "  inputs_cnn = Input(shape=(I), name='inputs_cnn')\n",
        "  conv1_1 = Convolution1D(filters=64,kernel_size=(1),padding=\"same\", activation=\"relu\")(inputs_cnn)\n",
        "  conv1_1 = Convolution1D(filters=64,kernel_size=(1),padding=\"same\", activation=\"relu\")(conv1_1)\n",
        "  conv1_1=BatchNormalization()(conv1_1)\n",
        "  pool1 = MaxPool1D(pool_size=(2),strides=(2))(conv1_1)\n",
        "  conv2_1 = Convolution1D(filters=64, kernel_size=(1), padding=\"same\", activation=\"relu\")(pool1)\n",
        "  conv2_1 = Convolution1D(filters=64, kernel_size=(1), padding=\"same\", activation=\"relu\")(conv2_1)\n",
        "  conv2_1=BatchNormalization()(conv2_1)\n",
        "  pool2 = MaxPool1D(pool_size=(2),strides=(2))(conv2_1)\n",
        "  conv3_1 = Convolution1D(filters=64, kernel_size=(1), padding=\"same\", activation=\"relu\")(pool2)  \n",
        "  conv3_1 = Convolution1D(filters=64, kernel_size=(1), padding=\"same\", activation=\"relu\")(conv3_1)\n",
        "  conv3_1 = Convolution1D(filters=64, kernel_size=(1), padding=\"same\", activation=\"relu\")(conv3_1)\n",
        "  conv3_1=BatchNormalization()(conv3_1)\n",
        "  pool3 = MaxPool1D(pool_size=(2),strides=(2))(conv3_1)\n",
        "  conv4_1 = Convolution1D(filters=64, kernel_size=(1), padding=\"same\", activation=\"relu\")(pool3)\n",
        "  conv4_1 = Convolution1D(filters=64, kernel_size=(1), padding=\"same\", activation=\"relu\")(conv4_1)\n",
        "  conv4_1 = Convolution1D(filters=64, kernel_size=(1), padding=\"same\", activation=\"relu\")(conv4_1)\n",
        "  conv4_1=BatchNormalization()(conv4_1)\n",
        "  pool4 = MaxPool1D(pool_size=(2),strides=(2))(conv4_1)\n",
        "  conv5_1 = Convolution1D(filters=64, kernel_size=(1), padding=\"same\", activation=\"relu\")(pool4)\n",
        "  conv5_1 = Convolution1D(filters=64, kernel_size=(1), padding=\"same\", activation=\"relu\")(conv5_1)\n",
        "  conv5_1 = Convolution1D(filters=64, kernel_size=(1), padding=\"same\", activation=\"relu\")(conv5_1)\n",
        "  conv5_1=BatchNormalization()(conv5_1)\n",
        "  pool5 = MaxPool1D(pool_size=(2),strides=(2))(conv5_1)\n",
        "  flatten = Flatten()(pool5)\n",
        "  dense1 = Dense(64, activation = 'relu')(flatten)\n",
        "  dense2 = Dense(32, activation = 'relu')(dense1)\n",
        "  main_output = Dense(1, activation = 'sigmoid', name = 'main_output')(dense2)\n",
        "\n",
        "  model = Model(inputs= inputs_cnn, outputs=main_output)\n",
        " \n",
        "  # lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(0.00001, decay_steps=10000, decay_rate=0.75)\n",
        "  # adam = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "  opt = Adam(lr=0.0001)\n",
        "  model.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
        "\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
        "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "\n",
        "  history=model.fit(X_train, y_train,epochs=40,callbacks=callbacks, batch_size=32,validation_data=(X_test,y_test))\n",
        "  model.load_weights('best_model.h5')\n",
        "  return(model,history)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   model = Sequential()\n",
        "# model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "# model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "# model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMUxa6PMRoWQ"
      },
      "source": [
        "def evaluate_model(history,X_test,y_test,model):\n",
        "    scores = model.evaluate((X_test),y_test, verbose=1)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    \n",
        "    print(history)\n",
        "    fig1, ax_acc = plt.subplots()\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Model - Accuracy')\n",
        "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "    plt.show()\n",
        "    \n",
        "    fig2, ax_loss = plt.subplots()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Model- Loss')\n",
        "    plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.show()\n",
        "    target_names=['0','1']\n",
        "    \n",
        "    y_true=[]\n",
        "    for element in y_test:\n",
        "        y_true.append(np.argmax(element))\n",
        "    prediction_proba=model.predict(X_test)\n",
        "    prediction=np.argmax(prediction_proba,axis=1)\n",
        "    cnf_matrix = confusion_matrix(y_true, prediction)\n",
        "    print(cnf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYUWocufRBXU"
      },
      "source": [
        "model, history = network(train_x, train_y, val_x, val_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_FfdqeaIfBp"
      },
      "source": [
        "# VGG\n",
        "model_vgg, history_vgg = make_vgg161D_model(train_x, train_y, val_x, val_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9P9qYEjd3B0"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "D1 = Dense(64,activation=\"relu\")(model.output)\n",
        "D2 = Dense(32, activation=\"relu\")(D1)\n",
        "O = Dense(1, activation='sigmoid')(D2)\n",
        "model_transfer = Model(inputs=model.input, outputs=O)\n",
        "\n",
        "for layer in model_transfer.layers[:-3]:\n",
        "    # layer.trainable = False\n",
        "    layer.trainable = True\n",
        "for layer in model_transfer.layers[-3:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "model_transfer.compile(optimizer=Adam(lr= 0.0001), loss='mse', metrics=['accuracy'])\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
        "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "\n",
        "# plot_model(model_transfer, show_shapes=True, to_file='VGG_transfer_learning.png')\n",
        "history_transfer = model_transfer.fit(train_x2, \n",
        "                    train_y2, \n",
        "                    validation_split=0.2,\n",
        "                    epochs=40,\n",
        "                    callbacks = callbacks,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(val_x2,val_y2),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax-Eh_Y5LdEb"
      },
      "source": [
        "# VGG transfer learning\n",
        "\n",
        "# Transfer Learning\n",
        "\n",
        "D1 = Dense(64,activation=\"relu\")(model_vgg.output)\n",
        "D2 = Dense(32, activation=\"relu\")(D1)\n",
        "O = Dense(1, activation='sigmoid')(D2)\n",
        "modelvgg_transfer = Model(inputs=model_vgg.input, outputs=O)\n",
        "\n",
        "for layer in modelvgg_transfer.layers[:-3]:\n",
        "    # layer.trainable = False\n",
        "    layer.trainable = True\n",
        "for layer in modelvgg_transfer.layers[-3:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "modelvgg_transfer.compile(optimizer=Adam(lr= 0.0001), loss='mse', metrics=['accuracy'])\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
        "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "\n",
        "# plot_model(model_transfer, show_shapes=True, to_file='VGG_transfer_learning.png')\n",
        "historyvgg_transfer = modelvgg_transfer.fit(train_x2, \n",
        "                    train_y2, \n",
        "                    validation_split=0.2,\n",
        "                    epochs=40,\n",
        "                    callbacks = callbacks,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(val_x2,val_y2),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dRwfKnvEWQ0"
      },
      "source": [
        "# bias and variance test\n",
        "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
        "        modelvgg_transfer, train_x2, train_y2, val_x2, val_y2, \n",
        "        loss='mse',\n",
        "        num_rounds=2,\n",
        "        random_seed=None,\n",
        "        epochs=40,\n",
        "        verbose=1) \n",
        "\n",
        "\n",
        "print('Average expected loss: %.3f' % avg_expected_loss)\n",
        "print('Average bias: %.3f' % avg_bias)\n",
        "print('Average variance: %.3f' % avg_var)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5JOPvTkMmEf"
      },
      "source": [
        "# bias and variance test\n",
        "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
        "        modelvgg_transfer, train_x2, train_y2, val_x2, val_y2, \n",
        "        loss='mse',\n",
        "        num_rounds=4,\n",
        "        random_seed=None,\n",
        "        epochs=10,\n",
        "        verbose=1) \n",
        "\n",
        "\n",
        "print('Average expected loss: %.3f' % avg_expected_loss)\n",
        "print('Average bias: %.3f' % avg_bias)\n",
        "print('Average variance: %.3f' % avg_var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLzSv4Q5NFtz"
      },
      "source": [
        "import math\n",
        "# yhat_classes = model_transfer.predict(val_x2, verbose=0)\n",
        "# yhat_classes = np.round(yhat_classes[:,0])\n",
        "# print(yhat_classes)\n",
        "\n",
        "# precision = precision_score(val_y2, yhat_classes)\n",
        "# print('Precision: %f' % precision)\n",
        "# # recall: tp / (tp + fn)\n",
        "# recall = recall_score(val_y2, yhat_classes)\n",
        "# print('Recall: %f' % recall)\n",
        "# # f1: 2 tp / (2 tp + fp + fn)\n",
        "# f1 = f1_score(val_y2, yhat_classes)\n",
        "# print('F1 score: %f' % f1)\n",
        " \n",
        "# # kappa\n",
        "# kappa = cohen_kappa_score(val_y2, yhat_classes)\n",
        "# print('Cohens kappa: %f' % kappa)\n",
        "# # ROC AUC\n",
        "# auc = roc_auc_score(val_y2, yhat_classes)\n",
        "# print('ROC AUC: %f' % auc)\n",
        "# # confusion matrix\n",
        "\n",
        "\n",
        "# AIC and BIC\n",
        "\n",
        "# Statistical test\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import std\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import log\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "# lower AIC and BIC means less information loss and overfitting\n",
        "def calculate_aic(n, mse, num_params):\n",
        "\taic = n * log(mse) + 2 * num_params\n",
        "\treturn aic\n",
        "\n",
        "\n",
        "def calculate_bic(n, mse, num_params):\n",
        "  bic = n * log(mse) + num_params * log(n)\n",
        "  return bic\n",
        "\n",
        "\n",
        "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
        "\n",
        "\n",
        "num_params = 50\n",
        "yhat = model_transfer.predict(val_x2, verbose = 0)\n",
        "yhat = np.round(yhat[:,0])\n",
        "mse_transfer_cnn = mean_squared_error(val_y2, yhat)\n",
        "print('MSE_CNN: %0.3f' % mse_transfer_cnn)\n",
        "\n",
        "bic_CNN = calculate_bic(len(train_y2), mse_transfer_cnn, num_params)\n",
        "print('BIC_CNN: %0.3f' % bic_CNN)\n",
        "\n",
        "aic_CNN = calculate_aic(len(train_y2), mse_transfer_cnn, num_params)\n",
        "print('AIC: %0.3f' % aic_CNN)\n",
        "\n",
        "\n",
        "num_params = 50\n",
        "yhat_vgg = modelvgg_transfer.predict(val_x2, verbose = 0)\n",
        "yhat_vgg = np.round(yhat_vgg[:,0])\n",
        "mse_transfer_vgg = mean_squared_error(val_y2, yhat_vgg)\n",
        "print('MSE_vgg: %0.3f' % mse_transfer_vgg)\n",
        "\n",
        "bic_vgg = calculate_bic(len(train_y2), mse_transfer_vgg, num_params)\n",
        "print('BIC_vgg: %0.3f' % bic_vgg)\n",
        "\n",
        "aic_vgg = calculate_aic(len(train_y2), mse_transfer_vgg, num_params)\n",
        "print('AIC_vgg: %0.3f' % aic_vgg)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcGWvJTGmTGe"
      },
      "source": [
        "def plot_learning(history):\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.subplot(211)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Acc using 1D VGG16 algorithm, autoencoder and transfer learning (swell input, wesad output)')\n",
        "    plt.legend([\"accuracy\", 'val_accuracy'])\n",
        "    fig2, ax_loss = plt.subplots()\n",
        "    plt.subplot(212)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'], label = \"val_loss\")\n",
        "    plt.title('Loss using 1D VGG16, autoencoder and transfer learning (swell input, wesad output)')\n",
        "    plt.legend([\"loss\", \"val_loss\"])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hRmjzGfmUdH"
      },
      "source": [
        "plot_learning(history_transfer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHynaGkwR4QY"
      },
      "source": [
        "model,history=network(train_x,train_y,val_x,val_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXr4LRZGVyC0"
      },
      "source": [
        "model2,history2=network(train_x2,train_y2,val_x2,val_y2)\n",
        "# plot_model(model2,  show_shapes=True, to_file='CNN_autoencoder.png')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCJ7eC_FShDG"
      },
      "source": [
        "evaluate_model(history_transfer,val_x2,val_y2,model_transfer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmlyjGJAWD_y"
      },
      "source": [
        "evaluate_model(history2,val_x2,val_y2,model2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQdXGiSX1O_b"
      },
      "source": [
        "Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6UPOOcr1M9o"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "%config IPCompleter.greedy=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjpG0O-j484m"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv1D, MaxPool1D, Activation, Add, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaHNF-uk5CmF"
      },
      "source": [
        "# input_shape = (50, 1)\n",
        "input_shape = (8,6,1)\n",
        "\n",
        "def make_model(final_layer_size=1):\n",
        "    I = Input(input_shape)\n",
        "    C = Conv1D(filters=32, kernel_size=2)(I)\n",
        "\n",
        "    C11 = Conv1D(filters=32, kernel_size=2, activation='relu', padding='same')(C)\n",
        "    C12 = Conv1D(filters=32, kernel_size=2, padding='same')(C11)\n",
        "    A11 = Add()([C, C12])\n",
        "    R11 = Activation(activation='relu')(A11)\n",
        "    M11 = MaxPool1D(pool_size=2, strides=2)(R11)\n",
        "\n",
        "    C21 = Conv1D(filters=32, kernel_size=2, activation='relu', padding='same')(M11)\n",
        "    C22 = Conv1D(filters=32, kernel_size=2, padding='same')(C21)\n",
        "    A21 = Add()([M11, C22])\n",
        "    R21 = Activation(activation='relu')(A21)\n",
        "    M21 = MaxPool1D(pool_size=2, strides=2)(R21)\n",
        "\n",
        "    C31 = Conv1D(filters=32, kernel_size=2, activation='relu', padding='same')(M21)\n",
        "    C32 = Conv1D(filters=32, kernel_size=2, padding='same')(C31)\n",
        "    A31 = Add()([M21, C32])\n",
        "    R31 = Activation(activation='relu')(A31)\n",
        "    M31 = MaxPool1D(pool_size=2, strides=2)(R31)\n",
        "\n",
        "    C41 = Conv1D(filters=32, kernel_size=2, activation='relu', padding='same')(M31)\n",
        "    C42 = Conv1D(filters=32, kernel_size=2, padding='same')(C41)\n",
        "    A41 = Add()([M31, C42])\n",
        "    R41 = Activation(activation='relu')(A41)\n",
        "    M41 = MaxPool1D(pool_size=2, strides=2)(R41)\n",
        "\n",
        "    C51 = Conv1D(filters=32, kernel_size=2, activation='relu', padding='same')(M41)\n",
        "    C52 = Conv1D(filters=32, kernel_size=2, padding='same')(C51)\n",
        "    A51 = Add()([M41, C52])\n",
        "    R51 = Activation(activation='relu')(A51)\n",
        "    M51 = MaxPool1D(pool_size=2, strides=2)(R51)\n",
        "\n",
        "    F1 = Flatten()(M51)\n",
        "    D1 = Dense(32)(F1)\n",
        "    R1 = Activation(activation='relu')(D1)\n",
        "    D2 = Dense(32)(R1)\n",
        "    D3 = Dense(final_layer_size)(D2)\n",
        "\n",
        "    O = Activation(activation='sigmoid')(D3)\n",
        "\n",
        "    return Model(inputs=I, outputs=O)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hatm8N28HJJI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKoEAOwSLCnJ"
      },
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(0.00001, decay_steps=10000, decay_rate=0.75)\n",
        "adam = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, amsgrad=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKdjH641LFXT"
      },
      "source": [
        "n_classes = len(np.unique(y2))\n",
        "print(n_classes)\n",
        "final_layer = 1\n",
        "model_2 = make_model(final_layer)\n",
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7zxe_L-YFu5"
      },
      "source": [
        "model_2.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAf9egYEYOEJ"
      },
      "source": [
        "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
        "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "\n",
        "history_2 = model_2.fit(np.expand_dims(train_x2, axis=2), \n",
        "                          train_y2, \n",
        "                          validation_split=0.20,\n",
        "                          callbacks=callbacks,  \n",
        "                          epochs=30,\n",
        "                          batch_size=8,\n",
        "                          verbose=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF9pBzpKgOZD"
      },
      "source": [
        "# Train data\n",
        "history_1 = model_2.fit(np.expand_dims(train_x, axis=2), \n",
        "                           train_y, \n",
        "                           validation_split=0.15,\n",
        "                           epochs=30,\n",
        "                           batch_size=256,\n",
        "                           verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASOU20zcdtRs"
      },
      "source": [
        "def plot_learning(history):\n",
        "    plt.subplot(211)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.legend([\"accuracy\", 'val_accuracy'])\n",
        "    plt.subplot(212)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'], label = \"val_loss\")\n",
        "    plt.legend([\"loss\", \"val_loss\"])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBpqQHrYgikt"
      },
      "source": [
        "plot_learning(history_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1B9j4ucdvHt"
      },
      "source": [
        "model.load_weights('best_model.h5')\n",
        "plot_learning(history_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAlsgKd_g6BT"
      },
      "source": [
        "# Results Train Data\n",
        "result_1 = model_2.evaluate(np.expand_dims(val_x, axis=2), val_y, batch_size=128)\n",
        "print(f\"The accuracy on the testing set is {np.round(result_1[1]*100,1)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9k0hiCwhHM6"
      },
      "source": [
        "# Results Test Data\n",
        "\n",
        "result_2 = model_2.evaluate(np.expand_dims(val_x2, axis=2), val_y2, batch_size=128)\n",
        "print(f\"The accuracy on the testing set is {np.round(result_2[1]*100,1)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9cRriHkh5sE"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "\n",
        "D1 = Dense(32)(model2.output)\n",
        "D2 = Dense(32)(D1)\n",
        "O = Dense(1, activation='sigmoid')(D2)\n",
        "model_transfer = Model(inputs=model2.input, outputs=O)\n",
        "\n",
        "for layer in model_transfer.layers[:-3]:\n",
        "    # layer.trainable = False\n",
        "    layer.trainable = True\n",
        "\n",
        "for layer in model_transfer.layers[-3:]:\n",
        "    layer.trainable = True\n",
        "    \n",
        "model_transfer.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
        "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "\n",
        "\n",
        "history_transfer = model_transfer.fit(np.expand_dims(train_x2, axis=2), \n",
        "                    train_y2, \n",
        "                    validation_split=0.2,\n",
        "                    epochs=30,\n",
        "                    callbacks = callbacks,\n",
        "                    batch_size=8,\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPEAegBCkpNu"
      },
      "source": [
        "model_transfer.load_weights('best_model.h5')\n",
        "plot_learning(history_transfer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5APlRp52k1Ta"
      },
      "source": [
        "print(\"Trainability of the layers \\n\")\n",
        "for layer in model_transfer.layers:\n",
        "    config = layer.get_config()\n",
        "    print(f\"{config['name']} : {config.get('trainable')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKsUmaVnk4XC"
      },
      "source": [
        "# Train Without Bottleneck\n",
        "\n",
        "D1 = Dense(32)(model_2.layers[-3].output)\n",
        "D2 = Dense(32)(D1)\n",
        "O = Dense(1, activation='sigmoid')(D2)\n",
        "model_transfer = Model(inputs=model_2.input, outputs=O)\n",
        "\n",
        "\n",
        "for layer in model_transfer.layers[:-3]:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in model_transfer.layers[-3:]:\n",
        "    layer.trainable = True\n",
        "    \n",
        "model_transfer.compile(optimizer=adam, loss='mse', metrics=['accuracy'])\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
        "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "history_transfer = model_transfer.fit(np.expand_dims(train_x2, axis=2), \n",
        "                    train_y2, \n",
        "                    validation_split=0.20,\n",
        "                    epochs=50,\n",
        "                    callbacks = callbacks,\n",
        "                    batch_size=8,\n",
        "                    verbose=1)\n",
        "\n",
        "plot_learning(history_transfer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdyBUGJvk9VS"
      },
      "source": [
        "# Without bottle neck and whole trainable model\n",
        "\n",
        "D1 = Dense(32)(model_2.layers[-3].output)\n",
        "D2 = Dense(32)(D1)\n",
        "O = Dense(1, activation='sigmoid')(D2)\n",
        "model_transfer = Model(inputs=model_2.input, outputs=O)\n",
        "model_transfer.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_transfer = model_transfer.fit(np.expand_dims(train_x, axis=2), \n",
        "                    train_y, \n",
        "                    validation_split=0.15,\n",
        "                    epochs=100,\n",
        "                    batch_size=8,\n",
        "                    verbose=0)\n",
        "plot_learning(history_transfer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFdjKiPqmvbr"
      },
      "source": [
        "# Transfer Learning with test data\n",
        "\n",
        "\n",
        "D1 = Dense(32)(model_2.output)\n",
        "D2 = Dense(32)(D1)\n",
        "O = Dense(1, activation='sigmoid')(D2)\n",
        "model_transfer2 = Model(inputs=model_2.input, outputs=O)\n",
        "\n",
        "for layer in model_transfer2.layers[:-3]:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in model_transfer2.layers[-3:]:\n",
        "    layer.trainable = True\n",
        "    \n",
        "model_transfer2.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history_transfer2 = model_transfer2.fit(np.expand_dims(train_x2, axis=2), \n",
        "                    train_y2, \n",
        "                    validation_split=0.15,\n",
        "                    epochs=30,\n",
        "                    batch_size=8,\n",
        "                    verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY5BSdHTnEP9"
      },
      "source": [
        "plot_learning(history_transfer2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbvrvo0HnNFG"
      },
      "source": [
        "# Test Without Bottleneck\n",
        "\n",
        "D1 = Dense(32)(model_2.layers[-3].output)\n",
        "D2 = Dense(32)(D1)\n",
        "O = Dense(1, activation='sigmoid')(D2)\n",
        "model_transfer2 = Model(inputs=model_2.input, outputs=O)\n",
        "\n",
        "\n",
        "for layer in model_transfer2.layers[:-3]:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in model_transfer2.layers[-3:]:\n",
        "    layer.trainable = True\n",
        "    \n",
        "model_transfer2.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_transfer2 = model_transfer2.fit(np.expand_dims(train_x2, axis=2), \n",
        "                    train_y2, \n",
        "                    validation_split=0.15,\n",
        "                    epochs=50,\n",
        "                    batch_size=8,\n",
        "                    verbose=1)\n",
        "\n",
        "plot_learning(history_transfer2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAvpc-5XoYpS"
      },
      "source": [
        "# Test without bottleneck and all trainable\n",
        "\n",
        "D1 = Dense(32)(model_2.layers[-3].output)\n",
        "D2 = Dense(32)(D1)\n",
        "O = Dense(1, activation='softmax')(D2)\n",
        "model_transfer2 = Model(inputs=model_2.input, outputs=O)\n",
        "model_transfer2.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_transfer2 = model_transfer2.fit(np.expand_dims(train_x2, axis=2), \n",
        "                    train_y2, \n",
        "                    validation_split=0.15,\n",
        "                    epochs=50,\n",
        "                    batch_size=8,\n",
        "                    verbose=1)\n",
        "plot_learning(history_transfer2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmfTQGcbuI1s"
      },
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import cross_val_score\n",
        "loocv = LeaveOneOut()\n",
        "results = cross_val_score(GB, rep_x2, rep_y2, cv = loocv)\n",
        "print(\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ3Px0_7AMZF"
      },
      "source": [
        "# sensitivity analysis of k in k-fold cross-validation\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from matplotlib import pyplot\n",
        "# Statistical test\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import std\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import log\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "# evaluate the model using a given test condition\n",
        "def evaluate_model(cv):\n",
        "\t# get the dataset\n",
        "\t# get the model\n",
        "\tRF = RandomForestClassifier() \n",
        "\t# evaluate the model\n",
        "\tscores = cross_val_score(RF, rep_xL, rep_yL, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\t# return scores\n",
        "\treturn mean(scores), scores.min(), scores.max()\n",
        "\n",
        "\n",
        "\n",
        "ideal, _, _ = evaluate_model(LeaveOneOut())\n",
        "print('Ideal: %0.3f' % ideal)\n",
        "\n",
        "\n",
        "\n",
        "# record mean and min/max of each set of results\n",
        "means = []\n",
        "mins = []\n",
        "maxs = []\n",
        "\n",
        "folds = range(2,31)\n",
        "# evaluate each k value\n",
        "\n",
        "for k in folds:\n",
        "\t# define the test condition\n",
        "\tcv = KFold(n_splits=k, shuffle=True, random_state=1)\n",
        "\t# evaluate k value\n",
        "\tk_mean, k_min, k_max = evaluate_model(cv)\n",
        "\t# report performance\n",
        "\tprint('> folds=%d, accuracy=%.3f (%.3f,%.3f)' % (k, k_mean, k_min, k_max))\n",
        "\t# store mean accuracy\n",
        "\tmeans.append(k_mean)\n",
        "\t# store min and max relative to the mean\n",
        "\tmins.append(k_mean - k_min)\n",
        "\tmaxs.append(k_max - k_mean)\n",
        " \n",
        "\n",
        "# line plot of k mean values with min/max error bars\n",
        "pyplot.errorbar(folds, means, yerr=[mins, maxs], fmt='o')\n",
        "# plot the ideal case in a separate color\n",
        "pyplot.plot(folds, [ideal for _ in range(len(folds))], color='r')\n",
        "# show the plot\n",
        "pyplot.title('Cross Validation Acc per Folds')\n",
        "pyplot.xlabel('Folds')\n",
        "pyplot.ylabel('Accuracy')\n",
        "pyplot.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDu_ITHSVoLm"
      },
      "source": [
        "# LOOCV vs CV\n",
        "# correlation between test harness and ideal test condition\n",
        "from numpy import mean\n",
        "from numpy import isnan\n",
        "from numpy import asarray\n",
        "from numpy import polyfit\n",
        "from scipy.stats import pearsonr\n",
        "from matplotlib import pyplot\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = []\n",
        "\tmodels.append(LogisticRegression())\n",
        "\tmodels.append(RidgeClassifier())\n",
        "\tmodels.append(SGDClassifier())\n",
        "\tmodels.append(PassiveAggressiveClassifier())\n",
        "\tmodels.append(KNeighborsClassifier())\n",
        "\tmodels.append(DecisionTreeClassifier())\n",
        "\tmodels.append(ExtraTreeClassifier())\n",
        "\tmodels.append(LinearSVC())\n",
        "\tmodels.append(SVC())\n",
        "\tmodels.append(GaussianNB())\n",
        "\tmodels.append(AdaBoostClassifier())\n",
        "\tmodels.append(BaggingClassifier())\n",
        "\tmodels.append(RandomForestClassifier())\n",
        "\tmodels.append(ExtraTreesClassifier())\n",
        "\tmodels.append(GaussianProcessClassifier())\n",
        "\tmodels.append(GradientBoostingClassifier())\n",
        "\tmodels.append(LinearDiscriminantAnalysis())\n",
        "\tmodels.append(QuadraticDiscriminantAnalysis())\n",
        "\treturn models\n",
        "\n",
        "# evaluate the model using a given test condition\n",
        "def evaluate_model(cv, model):\n",
        "\t# get the dataset\n",
        "\t# evaluate the model\n",
        "\tscores = cross_val_score(model, rep_xL, rep_yL, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\t# return scores\n",
        "\treturn mean(scores)\n",
        "\n",
        "# define test conditions\n",
        "ideal_cv = LeaveOneOut()\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "# get the list of models to consider\n",
        "models = get_models()\n",
        "# collect results\n",
        "ideal_results, cv_results = [], []\n",
        "# evaluate each model\n",
        "for model in models:\n",
        "\t# evaluate model using each test condition\n",
        "\tcv_mean = evaluate_model(cv, model)\n",
        "\tideal_mean = evaluate_model(ideal_cv, model)\n",
        "\t# check for invalid results\n",
        "\tif isnan(cv_mean) or isnan(ideal_mean):\n",
        "\t\tcontinue\n",
        "\t# store results\n",
        "\tcv_results.append(cv_mean)\n",
        "\tideal_results.append(ideal_mean)\n",
        "\t# summarize progress\n",
        "\tprint('>%s: ideal=%.3f, cv=%.3f' % (type(model).__name__, ideal_mean, cv_mean))\n",
        "# calculate the correlation between each test condition\n",
        "corr, _ = pearsonr(cv_results, ideal_results)\n",
        "print('Correlation: %.3f' % corr)\n",
        "# scatter plot of results\n",
        "pyplot.scatter(cv_results, ideal_results)\n",
        "# plot the line of best fit\n",
        "coeff, bias = polyfit(cv_results, ideal_results, 1)\n",
        "line = coeff * asarray(cv_results) + bias\n",
        "pyplot.plot(cv_results, line, color='r')\n",
        "# label the plot\n",
        "pyplot.title('10-fold CV vs LOOCV Mean Accuracy')\n",
        "pyplot.xlabel('Mean Accuracy (10-fold CV)')\n",
        "pyplot.ylabel('Mean Accuracy (LOOCV)')\n",
        "# show the plot\n",
        "pyplot.show()\n",
        "# correlation between test harness and ideal test condition\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "malxUXVlc22A"
      },
      "source": [
        "# Leave one out cross validation for deep learning algorithms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "# train_x2, val_x2, train_y2, val_y2\n",
        "# rep_x2, rep_y2\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "for train_index, test_index in loo.split(rep_x2):\n",
        "  train_x2, val_x2 = rep_x2[train_index], rep_x2[test_index]\n",
        "  train_y2, val_y2 = rep_y2[train_index], rep_y2[test_index]\n",
        "  \n",
        "  D1 = Dense(32)(model_vgg.output)\n",
        "  D2 = Dense(32)(D1)\n",
        "  O = Dense(1, activation='sigmoid')(D2)\n",
        "  modelvgg_transfer = Model(inputs=model_vgg.input, outputs=O)\n",
        "\n",
        "  for layer in modelvgg_transfer.layers[:-3]:\n",
        "    # layer.trainable = False\n",
        "    layer.trainable = True\n",
        "\n",
        "  for layer in modelvgg_transfer.layers[-3:]:\n",
        "    layer.trainable = True\n",
        "    \n",
        "  modelvgg_transfer.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
        "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "\n",
        "\n",
        "  history_transfer = modelvgg_transfer.fit(np.expand_dims(train_x2, axis=2), \n",
        "                    train_y2, \n",
        "                    validation_split=0.2,\n",
        "                    epochs=30,\n",
        "                    callbacks = callbacks,\n",
        "                    batch_size=8,\n",
        "                    verbose=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T_6TS9rHz_0"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "kfold = KFold(n_splits = 10, shuffle=True)\n",
        "\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "\n",
        "for train, test in kfold.split(rep_x2, rep_y2):\n",
        "  train_x2, val_x2 = rep_x2[train], rep_x2[test]\n",
        "  train_y2, val_y2 = rep_y2[train], rep_y2[test]\n",
        "\n",
        "  D1 = Dense(32)(model_vgg.output)\n",
        "  D2 = Dense(32)(D1)\n",
        "  O = Dense(1, activation='sigmoid')(D2)\n",
        "  modelvgg_transfer = Model(inputs=model_vgg.input, outputs=O)\n",
        "\n",
        "  for layer in modelvgg_transfer.layers[:-3]:\n",
        "    # layer.trainable = False\n",
        "    layer.trainable = True\n",
        "\n",
        "  for layer in modelvgg_transfer.layers[-3:]:\n",
        "    layer.trainable = True\n",
        "    \n",
        "  modelvgg_transfer.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
        "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  history_transfer = modelvgg_transfer.fit(np.expand_dims(train_x2, axis=2), \n",
        "                    train_y2, \n",
        "                    validation_split=0.2,\n",
        "                    epochs=30,\n",
        "                    callbacks = callbacks,\n",
        "                    batch_size=8,\n",
        "                    verbose=1)\n",
        "  \n",
        "  scores = modelvgg_transfer.evaluate(val_x2, val_y2, verbose = 1)\n",
        "\n",
        "  print(f'Score for fold {fold_no}: {modelvgg_transfer.metrics_names[0]} of {scores[0]}; {modelvgg_transfer.metrics_names[1]} of {scores[1]*100}%')\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}